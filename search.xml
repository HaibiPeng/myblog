<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Federated Learning and Split Learning Basics</title>
    <url>/2022/01/25/Federated-Learning-and-Split-Learning-Basics/</url>
    <content><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Often there needs to be built deep learning applications, that need a lot data, but this data might be available from multiple entities (humans, organizations). And this data might be sensitive, meaning the entities (humans or organizations) from whom we need the data might not want to share this data due to privacy reasons.</p>
<p>Federated learning (FL) and split learning (SL) are two popular distributed machine learning approaches, which can enable collaborative training of distributed machine learning models without any data sharing. Both follow a model-to-data scenario; clients train and test machine learning models without sharing raw data. SL provides better model privacy than FL due to the machine learning model architecture split between clients and the server. Moreover, the split model makes SL a better option for resource-constrained environments.</p>
<p>The blog introduces some basic concepts of FL and SL and it is recommended to check the references/sources at the end.</p>
<h1 id="Federated-Learning"><a href="#Federated-Learning" class="headerlink" title="Federated Learning"></a>Federated Learning</h1><h2 id="Concepts"><a href="#Concepts" class="headerlink" title="Concepts"></a>Concepts</h2><p><em><strong><a href="https://en.wikipedia.org/wiki/Federated_learning#Definition">Federated learning</a></strong></em> is a machine learning technique that trains an algorithm <strong>across multiple decentralized edge devices or servers</strong> holding local data samples, <strong>without exchanging them</strong>. </p>
<p>It aims at training a machine learning algorithm, for instance deep neural networks, <strong>on multiple local datasets contained in local nodes without explicitly exchanging data samples</strong>. The general principle consists in <strong>training local models on local data samples and exchanging parameters</strong> (e.g. the weights and biases of a deep neural network) between these local nodes at some frequency to generate a global model shared by all nodes.</p>
<ul>
<li>Data protection, telecommunications, IoT, and pharmaceutics are among the sectors where it’s used.</li>
</ul>
<h2 id="Categorization-of-Federated-Learning"><a href="#Categorization-of-Federated-Learning" class="headerlink" title="Categorization of Federated Learning"></a>Categorization of Federated Learning</h2><h3 id="Horizontal-federated-learning"><a href="#Horizontal-federated-learning" class="headerlink" title="Horizontal federated learning"></a>Horizontal federated learning</h3><p>Horizontal federated learning, or sample-based federated learning, is introduced in the scenarios that data sets share the same feature space but different in samples, i.e., the user features ( X1, X2, … ) of the two datasets have a large overlap, while the user ( U1, U2, … ) overlap is small.</p>
<figure>
    <img src="https://media.arxiv-vanity.com/render-output/5547782/HFL.png" alt="Categorization of Federated Learning">
    <figcaption align="center" style="font-size: 12px">(a) Horizontal Federated Learning</figcaption>
</figure>

<figure>
    <img src="https://media.arxiv-vanity.com/render-output/5547782/hfl_arc.png" alt="Architecture for a horizontal federated learning system">
    <figcaption align="center" style="font-size: 12px">Architecture for a horizontal federated learning system</figcaption>
</figure>

<h3 id="Vertical-federated-learning"><a href="#Vertical-federated-learning" class="headerlink" title="Vertical federated learning"></a>Vertical federated learning</h3><p>Vertical federated learning, or feature-based federated learning is applicable to the cases that two data sets share the same sample ID space but differ in feature space, i.e., the overlap of users ( U1, U2, … ) of the two datasets is large, while the overlap of user features ( X1, X2, … ) is small;</p>
<figure>
    <img src="https://media.arxiv-vanity.com/render-output/5547782/VFL.png" alt="Categorization of Federated Learning">
    <figcaption align="center" style="font-size: 12px">(b) Vertical Federated Learning</figcaption>
</figure>

<figure>
    <img src="https://media.arxiv-vanity.com/render-output/5547782/3.png" alt="Architecture for a vertical federated learning system">
    <figcaption align="center" style="font-size: 12px">Architecture for a vertical federated learning system</figcaption>
</figure>

<h3 id="Federated-transfer-learning"><a href="#Federated-transfer-learning" class="headerlink" title="Federated transfer learning"></a>Federated transfer learning</h3><p>Federated transfer learning applies to the scenarios that the two data sets differ not only in samples but also in feature space, i.e., solves the problem that the overlap between users ( U1, U2, … ) and user features ( X1, X2, … ) of the two datasets is relatively small.</p>
<figure>
    <img src="https://media.arxiv-vanity.com/render-output/5547782/FTL.png" alt="Categorization of Federated Learning">
    <figcaption align="center" style="font-size: 12px">(c) Federated Transfer Learning</figcaption>
</figure>

<h2 id="Benefits"><a href="#Benefits" class="headerlink" title="Benefits"></a>Benefits</h2><ul>
<li>FL allows devices such as smartphones to <em><strong>learn a shared prediction model collaboratively while maintaining the training data on the devices</strong></em> rather than uploading and storing it on a central server.</li>
<li>Moves model teaching to the edge, including gadgets like smartphones, laptops, IoT, and even “organizations” like hospitals that must work under stringent privacy regulations. It is a <em><strong>significant security advantage to keep personal data local</strong></em>.</li>
<li>Since prediction takes place on the system itself, <em><strong>real-time prediction is feasible</strong></em>. The time lag caused by sending raw data to a central server and then shipping the results back to the system is reduced by FL.</li>
<li>The amount of hardware equipment available is reduced by FL. FL versions need very little hardware, and what is available on mobile devices is more than adequate.</li>
</ul>
<h1 id="Split-Learning"><a href="#Split-Learning" class="headerlink" title="Split Learning"></a>Split Learning</h1><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p><em><strong><a href="https://scholar.google.com/scholar?hl=zh-CN&as_sdt=0,5&q=split+learning&btnG=">Split learning</a></strong></em> naturally allows for various configurations of cooperating entities to train (and infer from) machine learning models without sharing any raw data or detailed information about the model.</p>
<h2 id="Key-idea-and-steps"><a href="#Key-idea-and-steps" class="headerlink" title="Key idea and steps"></a>Key idea and steps</h2><p>In the simplest of configurations of split learning, let’s consider a client-server architecture. The steps are as follows.</p>
<h3 id="Forward-Propagation"><a href="#Forward-Propagation" class="headerlink" title="Forward Propagation"></a>Forward Propagation</h3><ol>
<li>Each client (for example, radiology center) trains a partial deep network up to a specific layer known as the <em><strong>cut layer</strong></em>. </li>
<li>The outputs at the cut layer are sent to another entity (server/another client, for example, computational entity) which completes the rest of the training <em><strong>without looking at raw data from any client that holds the raw data</strong></em>. This completes a round of <em><strong>forward propagation</strong></em> without sharing raw data. <h3 id="Backward-Propagation"><a href="#Backward-Propagation" class="headerlink" title="Backward Propagation"></a>Backward Propagation</h3></li>
<li>The gradients are now <em><strong>back propagated</strong></em> again from its last layer until the cut layer in a similar fashion. </li>
<li>The gradients at the cut layer (and only these gradients) are sent back to radiology client centers. <em><strong>The rest of back propagation</strong></em> is now completed at the radiology client centers. </li>
<li>This process is continued until the distributed split learning network is trained without looking at each others raw data.</li>
</ol>
<figure>
    <img src="SL.png" alt="Data flow between the client and the server in Algorithm 1 and 2">
    <figcaption align="center" style="font-size: 12px">Data flow between the client and the server in Algorithm 1 and 2</figcaption>
</figure>

<h2 id="Versatile-plug-and-play-configurations-of-split-learning"><a href="#Versatile-plug-and-play-configurations-of-split-learning" class="headerlink" title="Versatile plug-and-play configurations of split learning"></a>Versatile plug-and-play configurations of split learning</h2><p>Versatile configurations of split learning configurations cater to various practical settings of<br>i) multiple entities holding different modalities of patient data,<br>ii) centralized and local health entities collaborating on multiple tasks,<br>iii) learning without sharing labels,<br>iv) multi-task split learning,<br>v) multi-hop split learning and other hybrid possibilities to name a few as shown below and further detailed in this <a href="https://arxiv.org/pdf/1812.00564.pdf">paper</a>.</p>
<figure>
    <img src="https://dam-prod.media.mit.edu/thumb/2018/12/11/splitConfig_C4njd4y.png.1400x1400.png" alt="Versatile configurations of split learning configurations">
    <figcaption align="center" style="font-size: 12px">Versatile configurations of split learning configurations</figcaption>
</figure>

<figure>
    <img src="https://miro.medium.com/max/1400/1*G70zWOQ8e6YgBsx641Obmw.png" alt="SplitNN Configurations with and without label sharing">
    <figcaption align="center" style="font-size: 12px">SplitNN Configurations with and without label sharing</figcaption>
</figure>

<h2 id="Benefits-1"><a href="#Benefits-1" class="headerlink" title="Benefits"></a>Benefits</h2><p><em><strong>Client-side communication costs are significantly reduced</strong></em> as the data to be transmitted is restricted to initial layers of the split learning network (splitNN) prior to the split. <em><strong>The client-side computation costs of learning the weights of the network are also significantly reduced</strong></em> for the same reason. In terms of model performance, <em><strong>the accuracies of Split NN remained competitive</strong></em> to other distributed deep learning methods like federated learning and large batch synchronous SGD with a drastically smaller client side computational burden when training on a larger number of clients.</p>
<figure>
    <img src="https://miro.medium.com/max/1400/1*Jdy8C7tCZHlBFjVz5xKvCA.png" alt="Split Learning advantages">
    <figcaption align="center" style="font-size: 12px">Split Learning advantages</figcaption>
</figure>

<h2 id="References-Sources"><a href="#References-Sources" class="headerlink" title="References/Sources"></a>References/Sources</h2><ol>
<li><a href="https://en.wikipedia.org/wiki/Federated_learning#Definition">https://en.wikipedia.org/wiki/Federated_learning#Definition</a></li>
<li><a href="https://www.xenonstack.com/blog/edge-ai-vs-federated-learning">https://www.xenonstack.com/blog/edge-ai-vs-federated-learning</a></li>
<li><a href="https://blog.csdn.net/weixin_45439861/article/details/100670390">https://blog.csdn.net/weixin_45439861/article/details/100670390</a></li>
<li><a href="https://www.arxiv-vanity.com/papers/1902.04885/">https://www.arxiv-vanity.com/papers/1902.04885/</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/79284686?utm_source=wechat_session">https://zhuanlan.zhihu.com/p/79284686?utm_source=wechat_session</a></li>
<li><a href="https://blog.csdn.net/weixin_45439861/article/details/100670390">https://blog.csdn.net/weixin_45439861/article/details/100670390</a></li>
<li>Yang, Qiang, et al. “Federated machine learning: Concept and applications.” ACM Transactions on Intelligent Systems and Technology (TIST) 10.2 (2019): 1-19.</li>
<li>Liu, Yang, et al. “A secure federated transfer learning framework.” IEEE Intelligent Systems 35.4 (2020): 70-82.</li>
<li>Chen, Yiqiang, et al. “Fedhealth: A federated transfer learning framework for wearable healthcare.” IEEE Intelligent Systems 35.4 (2020): 83-93.</li>
<li>Li, Tian, et al. “Federated learning: Challenges, methods, and future directions.” IEEE Signal Processing Magazine 37.3 (2020): 50-60.</li>
<li><a href="https://www.media.mit.edu/projects/distributed-learning-and-collaborative-learning-1/overview/">https://www.media.mit.edu/projects/distributed-learning-and-collaborative-learning-1/overview/</a></li>
<li><a href="http://splitlearning.mit.edu/">http://splitlearning.mit.edu/</a></li>
<li>Abuadbba, Sharif, et al. “Can we use split learning on 1d cnn models for privacy preserving training?.” Proceedings of the 15th ACM Asia Conference on Computer and Communications Security. 2020.</li>
<li>Vepakomma, Praneeth, et al. “Split learning for health: Distributed deep learning without sharing raw patient data.” arXiv preprint arXiv:1812.00564 (2018).</li>
<li>Vepakomma, Praneeth, et al. “No peek: A survey of private distributed deep learning.” arXiv preprint arXiv:1812.03288 (2018).</li>
<li><a href="https://liveramp.com/developers/blog/privacy-preserving-split-learning/">https://liveramp.com/developers/blog/privacy-preserving-split-learning/</a></li>
<li>Gupta, Otkrist, and Ramesh Raskar. “Distributed learning of deep neural network over multiple agents.” Journal of Network and Computer Applications 116 (2018): 1-8.</li>
<li>Vepakomma, Praneeth, et al. “No peek: A survey of private distributed deep learning.” arXiv preprint arXiv:1812.03288 (2018).</li>
</ol>
]]></content>
      <categories>
        <category>Machine Learning</category>
        <category>Deep Learning</category>
        <category>Distributed Systems</category>
      </categories>
      <tags>
        <tag>Machine Learning, Deep Learning, Distributed Systems, Security and Privacy</tag>
      </tags>
  </entry>
  <entry>
    <title>Notes on Transfer Learning</title>
    <url>/2022/01/23/Notes-on-Transfer-Learning/</url>
    <content><![CDATA[<h1 id="Transfer-Learning"><a href="#Transfer-Learning" class="headerlink" title="Transfer Learning"></a>Transfer Learning</h1><h2 id="Concepts"><a href="#Concepts" class="headerlink" title="Concepts"></a>Concepts</h2><h3 id="1-Definition"><a href="#1-Definition" class="headerlink" title="1. Definition"></a>1. Definition</h3><p><a href="https://en.wikipedia.org/wiki/Transfer_learning">Transfer learning (TL)</a> is a research problem in machine learning (ML) that focuses on <strong>storing knowledge gained while solving one problem and applying it to a different but related problem</strong>.</p>
<p>It is a machine-learning method where the application of knowledge <strong>obtained from a model used in one task</strong> can be <strong>reused as a foundation point</strong> for another task.</p>
<ul>
<li>Ability of a system to recognize and apply knowledge and skills learned in previous domains/tasks to novel domains/tasks.</li>
</ul>
<h3 id="2-Domain-and-Task"><a href="#2-Domain-and-Task" class="headerlink" title="2. Domain and Task"></a>2. Domain and Task</h3><ul>
<li><p>Domain: consists of: a <a href="https://en.wikipedia.org/wiki/Feature_space">feature space</a> X and a <a href="https://en.wikipedia.org/wiki/Marginal_distribution">marginal probability distribution</a> P(X)</p>
</li>
<li><p>Task: consists of two components: a label space Y and an objective predictive function f: X → Y</p>
</li>
</ul>
<h2 id="Different-Types-of-Transfer-Learning"><a href="#Different-Types-of-Transfer-Learning" class="headerlink" title="Different Types of Transfer Learning"></a>Different Types of Transfer Learning</h2><p>(1) <strong>Inductive Transfer Learning (归纳式迁移学习).</strong> The source and target <em><strong>domains are the same(domains一样)</strong></em>, however, their <em><strong>tasks are still different(tasks不一样)</strong></em> from each another. The model will use <em><strong>inductive biases</strong></em> from the source domain to help improve the performance of the target task. The source task <em><strong>may or may not contain labeled data</strong></em>, further leading onto the model using multitask learning and self-taught learning.</p>
<ul>
<li><strong>multitask learning（多任务学习）</strong>：source domain的labeled数据可得。</li>
<li><strong>self-taught learning（自学习）</strong>：source domain的labeled数据不可得。</li>
</ul>
<p>(2) <strong>Transductive Transfer Learning (直推式迁移学习).</strong> The source and target <em><strong>tasks share similarities(tasks类似)</strong></em>, however, the <em><strong>domains are different(domains不一样)</strong></em>. The source domain contains a lot of <em><strong>labeled data</strong></em>, whereas there is <em><strong>an absence of labeled</strong></em> data in the target domain, further leading onto the model using domain adaptation.</p>
<p>Based on the number of domains and tasks, it can also be further divided into two types:</p>
<ul>
<li><strong>Domain Adaptation（域适配）</strong>：不同的domains+single task</li>
<li><strong>Sample Selection Bias（样本选择偏差 / Covariance Shift（协方差转变）</strong>：single domain+single task</li>
</ul>
<p>(3) <strong>Unsupervised Transfer Learning (无监督迁移学习).</strong> Unsupervised learning is when an algorithm is <em><strong>subjected to being able to identify patterns in data sets that have not been labeled or classified</strong></em>. In this case, the source and target <em><strong>domains are similar(domains类似)</strong></em>, however, <em><strong>the tasks are different</strong></em>, where <em><strong>data is unlabeled in both source and target(都不可得)</strong></em>. Techniques such as dimensionality reduction and clustering are well known in unsupervised learning.</p>
<p>Summarization the different settings and scenarios for each of the above techniques in the following table.</p>
<figure>
    <img src="https://miro.medium.com/max/2000/1*ZEJeJS06czdyPwov5EbCuQ.png" alt="Types of Transfer Learning Strategies and their Settings">
    <figcaption align="center" style="font-size: 12px">Types of Transfer Learning Strategies and their Settings</figcaption>
</figure>


<h2 id="What-to-transfer"><a href="#What-to-transfer" class="headerlink" title="What to transfer"></a>What to transfer</h2><h3 id="1-Homogeneous-Transfer-Learning-同构迁移学习"><a href="#1-Homogeneous-Transfer-Learning-同构迁移学习" class="headerlink" title="1. Homogeneous Transfer Learning (同构迁移学习)"></a>1. <strong>Homogeneous Transfer Learning</strong> (同构迁移学习)</h3><p>Homogeneous Transfer learning approaches are developed and proposed to handle situations where <em><strong>the domains are of the same feature space</strong></em>.</p>
<p>In Homogeneous Transfer learning, <em><strong>domains have only a slight difference in marginal distributions</strong></em>. These approaches adapt the domains by <em><strong>correcting the sample selection bias or covariate shift</strong></em>.</p>
<p>(1) <strong>Instance-based transfer</strong>（样本迁移）</p>
<p>It covers a simple scenario in which there is <em><strong>a large amount of labeled data in the source domain and a limited number in the target domain</strong></em>. Both the domains and feature spaces <em><strong>differ only in marginal distributions</strong></em>.</p>
<p>In this scenario, it is natural to consider <em><strong>adapting the marginal distributions</strong></em>. Instance-based Transfer learning <em><strong>reassigns weights to the source domain instances in the loss function</strong></em>.</p>
<p>Instance reweighting（样本重新调整权重） and importance sampling（重要性采样）are two main approaches used in instance-based TL.</p>
<p>(2) <strong>Feature-representation transfer</strong>（特征迁移）</p>
<p>Feature-based approaches transform the original features to create a new feature representation. This approach can further be divided into two subcategories, i.e., asymmetric and symmetric Feature-based Transfer Learning.</p>
<ul>
<li><strong>Asymmetric approaches</strong> transform the source features to match the target ones. In other words, we <em><strong>take the features from the source domain and fit them into the target feature space</strong></em>. There can be some information loss in this process due to the marginal difference in the feature distribution.</li>
<li><strong>Symmetric approaches</strong> find a common latent feature space and then transform both the source and the target features into this new feature representation.</li>
</ul>
<p>(3) <strong>Parameter transfer</strong>（参数/模型迁移）</p>
<p>The parameter-based transfer learning approaches transfer the knowledge at the <em><strong>model/parameter level</strong></em>.</p>
<p>This approach involves transferring knowledge through the shared parameters of the source and target domain learner models. One way to transfer the learned knowledge can be <em><strong>by creating multiple source learner models and optimally combining the re-weighted learners similar to ensemble learners to form an improved target learner</strong></em>.</p>
<p>The idea behind parameter-based methods is that <em><strong>a well-trained model on the source domain has learned a well-defined structure, and if two tasks are related, this structure can be transferred to the target model</strong></em>. In general, there are two ways to share the weights in deep learning models: </p>
<ul>
<li><strong>Soft weight sharing</strong>. The model is expected to be close to the already learned features and is usually penalized if its weights deviate significantly from a given set of weights.</li>
<li><strong>Hard weight sharing</strong>. We share the exact weights among different models.</li>
</ul>
<p>(4) <strong>Relational-knowledge transfer</strong>（关系迁移）</p>
<p>Relational-based transfer learning approaches mainly focus on <em><strong>learning the relations between the source and a target domain</strong></em> and <em><strong>using this knowledge to derive past knowledge and use it in the current context</strong></em>.</p>
<p>Such approaches transfer <em><strong>the logical relationship or rules learned in the source domain to the target domain</strong></em>.</p>
<p>For example, if we learn the relationship between different elements of the speech in a male voice, it can help significantly to analyze the sentence in another voice.</p>
<h3 id="2-Heterogeneous-Transfer-Learning-异构迁移学习"><a href="#2-Heterogeneous-Transfer-Learning-异构迁移学习" class="headerlink" title="2. Heterogeneous Transfer Learning (异构迁移学习)"></a>2. <strong>Heterogeneous Transfer Learning</strong> (异构迁移学习)</h3><p>It is often challenging to collect labeled source domain data with the same feature space as the target domain, and Heterogeneous Transfer learning methods are developed to address such limitations.  </p>
<p>This technique aims to <em><strong>solve the issue of source and target domains having differing feature spaces and other concerns like differing data distributions and label spaces</strong></em>. Heterogeneous Transfer Learning is applied in cross-domain tasks such as cross-language text categorization, text-to-image classification, and many others.</p>
<p>The following table clearly summarizes the relationship between different transfer learning strategies and what to transfer.</p>
<figure>
    <img src="https://miro.medium.com/max/700/1*xK81ohzG-tLRKVexowUvgw.png" alt="Transfer Learning Strategies and Types of Transferable Components">
    <figcaption align="center" style="font-size: 12px">Transfer Learning Strategies and Types of Transferable Components</figcaption>
</figure>

<figure>
    <img src="https://miro.medium.com/max/611/1*mEHO0-LifV7MgwXSpY9wyQ.png" alt="An overview of different settings of transfer">
    <figcaption align="center" style="font-size: 12px">An overview of different settings of transfer</figcaption>
</figure>

<h2 id="Models"><a href="#Models" class="headerlink" title="Models"></a>Models</h2><h3 id="For-computer-vision"><a href="#For-computer-vision" class="headerlink" title="For computer vision"></a>For computer vision</h3><ol>
<li>Xception</li>
<li>VGG16</li>
<li>VGG19</li>
<li>ResNet50</li>
<li>InceptionV3</li>
<li>InceptionResNetV2</li>
<li>MobileNet</li>
<li>MobileNetV2</li>
<li>DenseNetV2</li>
<li>DenseNet121</li>
<li>DenseNet169</li>
<li>DenseNet201</li>
<li>NASNetMobile</li>
<li>NASNetLarge</li>
</ol>
<h3 id="For-natural-language-processing"><a href="#For-natural-language-processing" class="headerlink" title="For natural language processing"></a>For natural language processing</h3><ol>
<li>Universal Sentence Encoder by Google</li>
<li>Bidirectional Encoder Representations from Transformers (BERT) by Google</li>
</ol>
<h3 id="For-sound-recognition"><a href="#For-sound-recognition" class="headerlink" title="For sound recognition"></a>For sound recognition</h3><ol>
<li>AudioSet</li>
<li>FreeSound</li>
<li>SoundWatch</li>
</ol>
<h1 id="References-sources"><a href="#References-sources" class="headerlink" title="References/sources"></a>References/sources</h1><ol>
<li><a href="https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a">A Comprehensive Hands-on Guide to Transfer Learning with Real-World Applications in Deep Learning</a></li>
<li><a href="https://www.v7labs.com/blog/transfer-learning-guide">A Newbie-Friendly Guide to Transfer Learning</a></li>
<li><a href="https://blog.csdn.net/vvnzhang2095/article/details/79882013?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.pc_relevant_default&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.pc_relevant_default&utm_relevant_index=2">迁移学习–综述</a></li>
<li>Pan S J, Yang Q. A survey on transfer learning[J]. IEEE Transactions on knowledge and data engineering, 2009, 22(10): 1345-1359.</li>
<li>Weiss K, Khoshgoftaar T M, Wang D D. A survey of transfer learning[J]. Journal of Big data, 2016, 3(1): 1-40.</li>
<li>Zhuang F, Qi Z, Duan K, et al. A comprehensive survey on transfer learning[J]. Proceedings of the IEEE, 2020, 109(1): 43-76.</li>
<li>Carney M, Webster B, Alvarado I, et al. Teachable machine: Approachable Web-based tool for exploring machine learning classification[C]//Extended abstracts of the 2020 CHI conference on human factors in computing systems. 2020: 1-8.</li>
<li>Goodman S M, Liu P, Jain D, et al. Toward user-driven sound recognizer personalization with people who are d/deaf or hard of hearing[J]. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 2021, 5(2): 1-23.</li>
<li>Laput G, Ahuja K, Goel M, et al. Ubicoustics: Plug-and-play acoustic activity recognition[C]//Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology. 2018: 213-224.</li>
<li><a href="https://github.com/jindongwang/transferlearning">https://github.com/jindongwang/transferlearning</a></li>
<li><a href="https://github.com/googlecreativelab/teachable-machine-boilerplate">https://github.com/googlecreativelab/teachable-machine-boilerplate</a></li>
</ol>
]]></content>
      <categories>
        <category>Machine Learning</category>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Transfer Learning, Machine Learning, Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Notes on Neural Network</title>
    <url>/2022/01/23/Notes-on-Neural-Network/</url>
    <content><![CDATA[<h1 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network"></a>Neural Network</h1><h2 id="Forward-Propagation"><a href="#Forward-Propagation" class="headerlink" title="Forward Propagation"></a><strong>Forward Propagation</strong></h2><p><em><strong><a href="https://www.youtube.com/watch?v=UJwK6jAStmg">Forward Propagation</a></strong></em> refers to the calculation and storage of intermediate variables (including outputs) for a neural network in order from the input layer to the output layer.</p>
<p>It is how neural networks make predictions. Input data is “forward propagated” through the network layer by layer to the final layer which outputs a prediction.</p>
<h3 id="Steps"><a href="#Steps" class="headerlink" title="Steps"></a>Steps</h3><ol>
<li>Calculate the weighted input to the hidden layer by multiplying 𝑋 by the hidden weight 𝑊ℎ</li>
<li>Apply the input of hidden layer to activation function and pass the result(output of hidden layer) to the final layer</li>
<li>Repeat step 2 except this time 𝑋 is replaced by the hidden layer’s output, 𝐻</li>
</ol>
<h2 id="Cost-Loss-Error-Function"><a href="#Cost-Loss-Error-Function" class="headerlink" title="Cost/Loss/Error Function"></a><strong><a href="https://en.wikipedia.org/wiki/Loss_function">Cost/Loss/Error Function</a></strong></h2><p>A loss function/error function(defined on a data point, prediction and label, and measures the penalty) is <em><strong>for a single training example/input</strong></em>. A cost function, on the other hand, is <em><strong>the average loss over the entire training dataset</strong></em>, which might be a sum of loss functions over your training set plus some model complexity penalty (regularization).<br>The optimization strategies aim at “minimizing the cost function”.</p>
<ul>
<li><p><a href="https://stats.stackexchange.com/questions/179026/objective-function-cost-function-loss-function-are-they-the-same-thing">Objective function, cost function, loss function: are they the same thing?</a></p>
</li>
<li><p>A loss function is a part of a cost function which is a type of an objective function.</p>
</li>
</ul>
<h2 id="Back-Propagation"><a href="#Back-Propagation" class="headerlink" title="Back Propagation"></a><strong>Back Propagation</strong></h2><p><em><strong><a href="https://www.cnblogs.com/charlotte77/p/5629865.html">Back Propagation</a></strong></em> refers to the method of calculating the gradient of neural network parameters. In short, the method traverses the network in reverse order, from the output to the input layer, according to the chain rule from calculus. The algorithm stores any intermediate variables (partial derivatives) required while calculating the gradient with respect to some parameters.</p>
<p>The goals of backpropagation are straightforward: <em><strong>adjust each weight in the network in proportion to how much it contributes to overall error</strong></em>. If we iteratively reduce each weight’s error, eventually we’ll have a series of weights that produce good predictions.</p>
<h3 id="Steps-1"><a href="#Steps-1" class="headerlink" title="Steps"></a>Steps</h3><ol>
<li>Compare the actual value output by the forward propagation process to the expected value</li>
<li>Moves backward through the network, slightly adjusting each of the weights in a direction that reduces the size of the error by a small degree</li>
<li>Both forward and back propagation are re-run thousands of times on each input combination until the network can accurately predict the expected output of the possible inputs using forward propagation.</li>
</ol>
<h3 id="Formulas-and-derivation"><a href="#Formulas-and-derivation" class="headerlink" title="Formulas and derivation"></a>Formulas and derivation</h3><ol>
<li><a href="https://blog.csdn.net/u014313009/article/details/51039334?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.pc_relevant_default&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.pc_relevant_default&utm_relevant_index=2">反向传播算法（过程及公式推导）</a></li>
<li><a href="https://brilliant.org/wiki/backpropagation/#">Backpropagation</a></li>
<li><a href="https://www.cnblogs.com/charlotte77/p/5629865.html">一文弄懂神经网络中的反向传播法——BackPropagation</a></li>
</ol>
<h2 id="Stochastic-Gradient-Descent-Algorithm"><a href="#Stochastic-Gradient-Descent-Algorithm" class="headerlink" title="Stochastic Gradient Descent Algorithm"></a><strong>Stochastic Gradient Descent Algorithm</strong></h2><p><em><strong>Gradient descent</strong></em> is an iterative algorithm, that starts from a random point on a function and travels down its slope in steps until it reaches the lowest point of that function.</p>
<p>Gradient descent <strong>can be slow to run on very large datasets</strong>.</p>
<p>Because one iteration of the gradient descent algorithm requires a prediction for each instance in the training dataset, it can <strong>take a long time when you have many millions of instances</strong>.</p>
<p><em><strong>Stochastic Gradient Descent(SGD)</strong></em> is a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). </p>
<p>In this variation, the gradient descent procedure is run but the update to the coefficients is performed for each training instance, rather than at the end of the batch of instances.</p>
<p>It is while selecting data points at each step to calculate the derivatives that induces randomness in gradient descent algorithm. SGD randomly picks one data point from the whole data set at each iteration to reduce the computations enormously.</p>
<ul>
<li><a href="https://www.analyticsvidhya.com/blog/2020/10/how-does-the-gradient-descent-algorithm-work-in-machine-learning/">How Does the Gradient Descent Algorithm Work in Machine Learning?</a></li>
<li><a href="https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31">Stochastic Gradient Descent — Clearly Explained !!</a></li>
<li><a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/">Gradient Descent For Machine Learning</a></li>
</ul>
<h2 id="Activation-Functions"><a href="#Activation-Functions" class="headerlink" title="Activation Functions"></a><strong>Activation Functions</strong></h2><ul>
<li><a href="https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#linear">Linear</a></li>
<li><a href="https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#elu">ELU</a></li>
<li><a href="https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#relu">ReLU</a></li>
<li><a href="https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#leakyrelu">LeakyReLU</a></li>
<li><a href="https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#sigmoid">Sigmoid</a></li>
<li><a href="https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#tanh">Tanh</a></li>
<li><a href="https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#softmax">Softmax</a></li>
</ul>
<h2 id="References-Sources"><a href="#References-Sources" class="headerlink" title="References/Sources"></a>References/Sources</h2><ol>
<li><a href="https://en.wikipedia.org/wiki/Gradient">https://en.wikipedia.org/wiki/Gradient</a></li>
<li><a href="https://en.wikipedia.org/wiki/Gradient_descent">https://en.wikipedia.org/wiki/Gradient_descent</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/34378516">神经网络（全连接）的前向和反向传播</a></li>
<li><a href="https://www.youtube.com/watch?v=UJwK6jAStmg">https://www.youtube.com/watch?v=UJwK6jAStmg</a></li>
<li><a href="https://en.wikipedia.org/wiki/Loss_function">https://en.wikipedia.org/wiki/Loss_function</a></li>
<li><a href="https://stats.stackexchange.com/questions/179026/objective-function-cost-function-loss-function-are-they-the-same-thing">https://stats.stackexchange.com/questions/179026/objective-function-cost-function-loss-function-are-they-the-same-thing</a></li>
<li><a href="https://blog.csdn.net/u014313009/article/details/51039334?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.pc_relevant_default&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.pc_relevant_default&utm_relevant_index=2">反向传播算法（过程及公式推导）</a></li>
<li><a href="https://brilliant.org/wiki/backpropagation/#">Backpropagation</a></li>
<li><a href="https://www.cnblogs.com/charlotte77/p/5629865.html">一文弄懂神经网络中的反向传播法——BackPropagation</a></li>
<li><a href="https://www.analyticsvidhya.com/blog/2020/10/how-does-the-gradient-descent-algorithm-work-in-machine-learning/">How Does the Gradient Descent Algorithm Work in Machine Learning?</a></li>
<li><a href="https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31">Stochastic Gradient Descent — Clearly Explained !!</a></li>
<li><a href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/">Gradient Descent For Machine Learning</a></li>
</ol>
]]></content>
      <categories>
        <category>Machine Learning</category>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Deep Learning, Neural Network</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo+GitHubPages+腾讯云搭建个人博客</title>
    <url>/2022/01/17/blog-how-to/</url>
    <content><![CDATA[<p>本教程需要读者具有基础的编辑器(VSCode/WebStorm/其他…)和命令行知识，以及文档教程阅读和复现能力。以下大致说明以下步骤吧。</p>
<h2 id="Hexo"><a href="#Hexo" class="headerlink" title="Hexo"></a>Hexo</h2><h3 id="安装依赖项和Hexo"><a href="#安装依赖项和Hexo" class="headerlink" title="安装依赖项和Hexo"></a>安装依赖项和Hexo</h3><p>按照官方文档安装所需程序（Node.js/Git）。</p>
<p>安装指南传送门：</p>
<ol>
<li><a href="https://www.runoob.com/nodejs/nodejs-install-setup.html">Node.js</a></li>
<li><a href="https://www.runoob.com/git/git-install-setup.html">Git</a></li>
<li><a href="https://hexo.io/zh-cn/docs/#%E5%AE%89%E8%A3%85-Hexo">Hexo</a></li>
</ol>
<h3 id="使用Hexo建站并修改配置"><a href="#使用Hexo建站并修改配置" class="headerlink" title="使用Hexo建站并修改配置"></a>使用Hexo建站并修改配置</h3><p>详见官方文档：</p>
<ol>
<li><a href="https://hexo.io/zh-cn/docs/setup">建站</a></li>
<li><a href="https://hexo.io/zh-cn/docs/configuration">修改配置</a></li>
</ol>
<h3 id="启动网站"><a href="#启动网站" class="headerlink" title="启动网站"></a>启动网站</h3><p>在命令行运行以下命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>然后再浏览器输入 <a href="http://localhost:4000/">http://localhost:4000/</a> 就可以打开网站看到原始版的个人博客网站了。</p>
<h2 id="GitHub-Pages"><a href="#GitHub-Pages" class="headerlink" title="GitHub Pages"></a>GitHub Pages</h2><p>这一部分需要你有一个github的账号，以及新建一个repo，并且将上一步Hexo中的代码关联到这个repo。</p>
<h3 id="创建GitHub-Pages站点"><a href="#创建GitHub-Pages站点" class="headerlink" title="创建GitHub Pages站点"></a>创建GitHub Pages站点</h3><p>指南传送门：<a href="https://docs.github.com/cn/pages/getting-started-with-github-pages/creating-a-github-pages-site">创建GitHub Pages站点</a></p>
<h3 id="修改博客url"><a href="#修改博客url" class="headerlink" title="修改博客url"></a>修改博客url</h3><p>在_config.yml文件中修改url为你的repo的Github Pages的网址：</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># URL</span></span><br><span class="line"><span class="comment">## Set your site url here. For example, if you use GitHub Page, set url as &#x27;https://username.github.io/repo&#x27;</span></span><br><span class="line"><span class="attr">url:</span> <span class="string">https://username.github.io/repo</span></span><br></pre></td></tr></table></figure>

<h3 id="部署到GitHub-Pages"><a href="#部署到GitHub-Pages" class="headerlink" title="部署到GitHub Pages"></a>部署到GitHub Pages</h3><ol>
<li>安装 hexo-deployer-git。</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure>

<ol start="2">
<li><p>修改配置：</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">deploy:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">  <span class="attr">repo:</span> <span class="string">&lt;repository</span> <span class="string">url&gt;</span> <span class="comment">#repo的地址，如https://github.com/username/repo</span></span><br><span class="line">  <span class="attr">branch:</span> [<span class="string">branch</span>] <span class="comment">#需要部署的代码分支，如main、master，需要去掉[]</span></span><br><span class="line">  <span class="attr">message:</span> [<span class="string">message</span>] <span class="comment">#部署信息，可以随便填，需要去掉[]</span></span><br></pre></td></tr></table></figure></li>
<li><p>生成站点文件并推送至远程库</p>
</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo clean &amp;&amp; hexo deploy</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>在库设置（Repository Settings）中将默认分支设置为_config.yml配置中的分支名称就可以了。</li>
</ol>
<p>这样你就可以用GitHub Pages的url打开你的个人博客了。</p>
<p>更详细的指南请移步官方文档: <a href="https://hexo.io/zh-cn/docs/one-command-deployment">一键部署到GitHub</a></p>
<h2 id="腾讯云"><a href="#腾讯云" class="headerlink" title="腾讯云"></a>腾讯云</h2><h3 id="域名申请"><a href="#域名申请" class="headerlink" title="域名申请"></a>域名申请</h3><p>购买申请指南请移步：<a href="https://dnspod.cloud.tencent.com/">腾讯云购买域名</a></p>
<p>其中需要一堆实名认证过程，请按照官方文档操作。</p>
<h3 id="绑定到Github-Pages"><a href="#绑定到Github-Pages" class="headerlink" title="绑定到Github Pages"></a>绑定到Github Pages</h3><p>详见这篇博客: <a href="https://cloud.tencent.com/developer/article/1421879">GitHub Pages搭建的博客绑定域名</a></p>
<p>添加DNS解析部分详见这篇，需要用命令行ping一下自己的github page获取ip地址：<a href="https://cloud.tencent.com/developer/article/1454059">github pages绑定域名</a></p>
<h3 id="修改先前的配置"><a href="#修改先前的配置" class="headerlink" title="修改先前的配置"></a>修改先前的配置</h3><p>在第二步中将博客url设置成了自己的GitHub Pages的url，而既然我们已经有了自己的域名，就需要把之前的url换成我们自己的域名：</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># URL</span></span><br><span class="line"><span class="comment">## 你自己申请的域名</span></span><br><span class="line"><span class="attr">url:</span> <span class="string">https://username.cn</span></span><br></pre></td></tr></table></figure>

<p>大功告成。</p>
<h2 id="Hexo主题"><a href="#Hexo主题" class="headerlink" title="Hexo主题"></a>Hexo主题</h2><p>如果你觉得默认主题不太好看，是可以自己设计或者更换主题的。</p>
<p>官方文档：<a href="https://hexo.io/zh-cn/docs/themes">主题</a></p>
<p>官方还有一个<a href="https://hexo.io/themes/">主题库</a>，可以搜索自己觉得好看的主题，然后去主题的GitHub repo下载并移到自己博客的theme文件夹下，修改对应配置为这个主题的名字。</p>
<p>比如我用的主题为[ocean](<a href="https://github.com/zhwangart/hexo-theme-ocean.git">https://github.com/zhwangart/hexo-theme-ocean.git</a> themes/ocean)，就在root _config.yml 中选择 theme: ocean：</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">theme:</span> <span class="string">ocean</span></span><br></pre></td></tr></table></figure>

<p>其他个性化配置就需要参考每个主题的相关文档了，一般是主题的设计开发者写的，没有的话就需要自己摸索了。主题里的各种图标，图片，视频等等都是可以自己换的，可能需要使用者有一些css、html和markdown的基础，这里就不深入下去了，有兴趣的话各位自己学习吧哈哈哈。</p>
]]></content>
      <categories>
        <category>Hexo, 个人博客</category>
      </categories>
      <tags>
        <tag>Hexo, 个人博客</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2022/01/16/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
      <categories>
        <category>hello</category>
      </categories>
      <tags>
        <tag>hello</tag>
      </tags>
  </entry>
</search>
