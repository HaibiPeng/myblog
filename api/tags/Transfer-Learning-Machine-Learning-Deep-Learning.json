{"name":"Transfer-Learning-Machine-Learning-Deep-Learning","postlist":[{"title":"Notes on Transfer Learning","slug":"Notes-on-Transfer-Learning","date":"2022-01-22T23:21:34.000Z","updated":"2022-01-22T23:44:27.985Z","comments":true,"path":"api/articles/Notes-on-Transfer-Learning.json","excerpt":null,"keywords":"programmer, photographer, PHB, PhotograpHB","cover":"https://miro.medium.com/max/2000/1*ZEJeJS06czdyPwov5EbCuQ.png","content":"<h1 id=\"Transfer-Learning\"><a href=\"#Transfer-Learning\" class=\"headerlink\" title=\"Transfer Learning\"></a>Transfer Learning</h1><h2 id=\"Concepts\"><a href=\"#Concepts\" class=\"headerlink\" title=\"Concepts\"></a>Concepts</h2><h3 id=\"1-Definition\"><a href=\"#1-Definition\" class=\"headerlink\" title=\"1. Definition\"></a>1. Definition</h3><p><a href=\"https://en.wikipedia.org/wiki/Transfer_learning\">Transfer learning (TL)</a> is a research problem in machine learning (ML) that focuses on <strong>storing knowledge gained while solving one problem and applying it to a different but related problem</strong>.</p>\n<p>It is a machine-learning method where the application of knowledge <strong>obtained from a model used in one task</strong> can be <strong>reused as a foundation point</strong> for another task.</p>\n<ul>\n<li>Ability of a system to recognize and apply knowledge and skills learned in previous domains/tasks to novel domains/tasks.</li>\n</ul>\n<h3 id=\"2-Domain-and-Task\"><a href=\"#2-Domain-and-Task\" class=\"headerlink\" title=\"2. Domain and Task\"></a>2. Domain and Task</h3><ul>\n<li><p>Domain: consists of: a <a href=\"https://en.wikipedia.org/wiki/Feature_space\">feature space</a> X and a <a href=\"https://en.wikipedia.org/wiki/Marginal_distribution\">marginal probability distribution</a> P(X)</p>\n</li>\n<li><p>Task: consists of two components: a label space Y and an objective predictive function f: X → Y</p>\n</li>\n</ul>\n<h2 id=\"Different-Types-of-Transfer-Learning\"><a href=\"#Different-Types-of-Transfer-Learning\" class=\"headerlink\" title=\"Different Types of Transfer Learning\"></a>Different Types of Transfer Learning</h2><p>(1) <strong>Inductive Transfer Learning (归纳式迁移学习).</strong> The source and target <em><strong>domains are the same(domains一样)</strong></em>, however, their <em><strong>tasks are still different(tasks不一样)</strong></em> from each another. The model will use <em><strong>inductive biases</strong></em> from the source domain to help improve the performance of the target task. The source task <em><strong>may or may not contain labeled data</strong></em>, further leading onto the model using multitask learning and self-taught learning.</p>\n<ul>\n<li><strong>multitask learning（多任务学习）</strong>：source domain的labeled数据可得。</li>\n<li><strong>self-taught learning（自学习）</strong>：source domain的labeled数据不可得。</li>\n</ul>\n<p>(2) <strong>Transductive Transfer Learning (直推式迁移学习).</strong> The source and target <em><strong>tasks share similarities(tasks类似)</strong></em>, however, the <em><strong>domains are different(domains不一样)</strong></em>. The source domain contains a lot of <em><strong>labeled data</strong></em>, whereas there is <em><strong>an absence of labeled</strong></em> data in the target domain, further leading onto the model using domain adaptation.</p>\n<p>Based on the number of domains and tasks, it can also be further divided into two types:</p>\n<ul>\n<li><strong>Domain Adaptation（域适配）</strong>：不同的domains+single task</li>\n<li><strong>Sample Selection Bias（样本选择偏差 / Covariance Shift（协方差转变）</strong>：single domain+single task</li>\n</ul>\n<p>(3) <strong>Unsupervised Transfer Learning (无监督迁移学习).</strong> Unsupervised learning is when an algorithm is <em><strong>subjected to being able to identify patterns in data sets that have not been labeled or classified</strong></em>. In this case, the source and target <em><strong>domains are similar(domains类似)</strong></em>, however, <em><strong>the tasks are different</strong></em>, where <em><strong>data is unlabeled in both source and target(都不可得)</strong></em>. Techniques such as dimensionality reduction and clustering are well known in unsupervised learning.</p>\n<p>Summarization the different settings and scenarios for each of the above techniques in the following table.</p>\n<figure>\n    <img src=\"https://miro.medium.com/max/2000/1*ZEJeJS06czdyPwov5EbCuQ.png\" alt=\"Types of Transfer Learning Strategies and their Settings\">\n    <figcaption align=\"center\" style=\"font-size: 12px\">Types of Transfer Learning Strategies and their Settings</figcaption>\n</figure>\n\n\n<h2 id=\"What-to-transfer\"><a href=\"#What-to-transfer\" class=\"headerlink\" title=\"What to transfer\"></a>What to transfer</h2><h3 id=\"1-Homogeneous-Transfer-Learning-同构迁移学习\"><a href=\"#1-Homogeneous-Transfer-Learning-同构迁移学习\" class=\"headerlink\" title=\"1. Homogeneous Transfer Learning (同构迁移学习)\"></a>1. <strong>Homogeneous Transfer Learning</strong> (同构迁移学习)</h3><p>Homogeneous Transfer learning approaches are developed and proposed to handle situations where <em><strong>the domains are of the same feature space</strong></em>.</p>\n<p>In Homogeneous Transfer learning, <em><strong>domains have only a slight difference in marginal distributions</strong></em>. These approaches adapt the domains by <em><strong>correcting the sample selection bias or covariate shift</strong></em>.</p>\n<p>(1) <strong>Instance-based transfer</strong>（样本迁移）</p>\n<p>It covers a simple scenario in which there is <em><strong>a large amount of labeled data in the source domain and a limited number in the target domain</strong></em>. Both the domains and feature spaces <em><strong>differ only in marginal distributions</strong></em>.</p>\n<p>In this scenario, it is natural to consider <em><strong>adapting the marginal distributions</strong></em>. Instance-based Transfer learning <em><strong>reassigns weights to the source domain instances in the loss function</strong></em>.</p>\n<p>Instance reweighting（样本重新调整权重） and importance sampling（重要性采样）are two main approaches used in instance-based TL.</p>\n<p>(2) <strong>Feature-representation transfer</strong>（特征迁移）</p>\n<p>Feature-based approaches transform the original features to create a new feature representation. This approach can further be divided into two subcategories, i.e., asymmetric and symmetric Feature-based Transfer Learning.</p>\n<ul>\n<li><strong>Asymmetric approaches</strong> transform the source features to match the target ones. In other words, we <em><strong>take the features from the source domain and fit them into the target feature space</strong></em>. There can be some information loss in this process due to the marginal difference in the feature distribution.</li>\n<li><strong>Symmetric approaches</strong> find a common latent feature space and then transform both the source and the target features into this new feature representation.</li>\n</ul>\n<p>(3) <strong>Parameter transfer</strong>（参数/模型迁移）</p>\n<p>The parameter-based transfer learning approaches transfer the knowledge at the <em><strong>model/parameter level</strong></em>.</p>\n<p>This approach involves transferring knowledge through the shared parameters of the source and target domain learner models. One way to transfer the learned knowledge can be <em><strong>by creating multiple source learner models and optimally combining the re-weighted learners similar to ensemble learners to form an improved target learner</strong></em>.</p>\n<p>The idea behind parameter-based methods is that <em><strong>a well-trained model on the source domain has learned a well-defined structure, and if two tasks are related, this structure can be transferred to the target model</strong></em>. In general, there are two ways to share the weights in deep learning models: </p>\n<ul>\n<li><strong>Soft weight sharing</strong>. The model is expected to be close to the already learned features and is usually penalized if its weights deviate significantly from a given set of weights.</li>\n<li><strong>Hard weight sharing</strong>. We share the exact weights among different models.</li>\n</ul>\n<p>(4) <strong>Relational-knowledge transfer</strong>（关系迁移）</p>\n<p>Relational-based transfer learning approaches mainly focus on <em><strong>learning the relations between the source and a target domain</strong></em> and <em><strong>using this knowledge to derive past knowledge and use it in the current context</strong></em>.</p>\n<p>Such approaches transfer <em><strong>the logical relationship or rules learned in the source domain to the target domain</strong></em>.</p>\n<p>For example, if we learn the relationship between different elements of the speech in a male voice, it can help significantly to analyze the sentence in another voice.</p>\n<h3 id=\"2-Heterogeneous-Transfer-Learning-异构迁移学习\"><a href=\"#2-Heterogeneous-Transfer-Learning-异构迁移学习\" class=\"headerlink\" title=\"2. Heterogeneous Transfer Learning (异构迁移学习)\"></a>2. <strong>Heterogeneous Transfer Learning</strong> (异构迁移学习)</h3><p>It is often challenging to collect labeled source domain data with the same feature space as the target domain, and Heterogeneous Transfer learning methods are developed to address such limitations.  </p>\n<p>This technique aims to <em><strong>solve the issue of source and target domains having differing feature spaces and other concerns like differing data distributions and label spaces</strong></em>. Heterogeneous Transfer Learning is applied in cross-domain tasks such as cross-language text categorization, text-to-image classification, and many others.</p>\n<p>The following table clearly summarizes the relationship between different transfer learning strategies and what to transfer.</p>\n<figure>\n    <img src=\"https://miro.medium.com/max/700/1*xK81ohzG-tLRKVexowUvgw.png\" alt=\"Transfer Learning Strategies and Types of Transferable Components\">\n    <figcaption align=\"center\" style=\"font-size: 12px\">Transfer Learning Strategies and Types of Transferable Components</figcaption>\n</figure>\n\n<figure>\n    <img src=\"https://miro.medium.com/max/611/1*mEHO0-LifV7MgwXSpY9wyQ.png\" alt=\"An overview of different settings of transfer\">\n    <figcaption align=\"center\" style=\"font-size: 12px\">An overview of different settings of transfer</figcaption>\n</figure>\n\n<h2 id=\"Models\"><a href=\"#Models\" class=\"headerlink\" title=\"Models\"></a>Models</h2><h3 id=\"For-computer-vision\"><a href=\"#For-computer-vision\" class=\"headerlink\" title=\"For computer vision\"></a>For computer vision</h3><ol>\n<li>Xception</li>\n<li>VGG16</li>\n<li>VGG19</li>\n<li>ResNet50</li>\n<li>InceptionV3</li>\n<li>InceptionResNetV2</li>\n<li>MobileNet</li>\n<li>MobileNetV2</li>\n<li>DenseNetV2</li>\n<li>DenseNet121</li>\n<li>DenseNet169</li>\n<li>DenseNet201</li>\n<li>NASNetMobile</li>\n<li>NASNetLarge</li>\n</ol>\n<h3 id=\"For-natural-language-processing\"><a href=\"#For-natural-language-processing\" class=\"headerlink\" title=\"For natural language processing\"></a>For natural language processing</h3><ol>\n<li>Universal Sentence Encoder by Google</li>\n<li>Bidirectional Encoder Representations from Transformers (BERT) by Google</li>\n</ol>\n<h3 id=\"For-sound-recognition\"><a href=\"#For-sound-recognition\" class=\"headerlink\" title=\"For sound recognition\"></a>For sound recognition</h3><ol>\n<li>AudioSet</li>\n<li>FreeSound</li>\n<li>SoundWatch</li>\n</ol>\n<h1 id=\"References-sources\"><a href=\"#References-sources\" class=\"headerlink\" title=\"References/sources\"></a>References/sources</h1><ol>\n<li><a href=\"https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a\">A Comprehensive Hands-on Guide to Transfer Learning with Real-World Applications in Deep Learning</a></li>\n<li><a href=\"https://www.v7labs.com/blog/transfer-learning-guide\">A Newbie-Friendly Guide to Transfer Learning</a></li>\n<li><a href=\"https://blog.csdn.net/vvnzhang2095/article/details/79882013?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.pc_relevant_default&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.pc_relevant_default&utm_relevant_index=2\">迁移学习–综述</a></li>\n<li>Pan S J, Yang Q. A survey on transfer learning[J]. IEEE Transactions on knowledge and data engineering, 2009, 22(10): 1345-1359.</li>\n<li>Weiss K, Khoshgoftaar T M, Wang D D. A survey of transfer learning[J]. Journal of Big data, 2016, 3(1): 1-40.</li>\n<li>Zhuang F, Qi Z, Duan K, et al. A comprehensive survey on transfer learning[J]. Proceedings of the IEEE, 2020, 109(1): 43-76.</li>\n<li>Carney M, Webster B, Alvarado I, et al. Teachable machine: Approachable Web-based tool for exploring machine learning classification[C]//Extended abstracts of the 2020 CHI conference on human factors in computing systems. 2020: 1-8.</li>\n<li>Goodman S M, Liu P, Jain D, et al. Toward user-driven sound recognizer personalization with people who are d/deaf or hard of hearing[J]. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 2021, 5(2): 1-23.</li>\n<li>Laput G, Ahuja K, Goel M, et al. Ubicoustics: Plug-and-play acoustic activity recognition[C]//Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology. 2018: 213-224.</li>\n<li><a href=\"https://github.com/jindongwang/transferlearning\">https://github.com/jindongwang/transferlearning</a></li>\n<li><a href=\"https://github.com/googlecreativelab/teachable-machine-boilerplate\">https://github.com/googlecreativelab/teachable-machine-boilerplate</a></li>\n</ol>\n","raw":null,"categories":[{"name":"Machine-Learning","path":"api/categories/Machine-Learning.json"},{"name":"Deep-Learning","path":"api/categories/Deep-Learning.json"}],"tags":[{"name":"Transfer-Learning-Machine-Learning-Deep-Learning","path":"api/tags/Transfer-Learning-Machine-Learning-Deep-Learning.json"}]}]}