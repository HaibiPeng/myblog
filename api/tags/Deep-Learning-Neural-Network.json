{"name":"Deep-Learning-Neural-Network","postlist":[{"title":"Notes on Neural Network","slug":"Notes-on-Neural-Network","date":"2022-01-22T23:20:43.000Z","updated":"2022-01-22T23:44:22.840Z","comments":true,"path":"api/articles/Notes-on-Neural-Network.json","excerpt":null,"keywords":"programmer, photographer, PHB, PhotograpHB","cover":null,"content":"<h1 id=\"Neural-Network\"><a href=\"#Neural-Network\" class=\"headerlink\" title=\"Neural Network\"></a>Neural Network</h1><h2 id=\"Forward-Propagation\"><a href=\"#Forward-Propagation\" class=\"headerlink\" title=\"Forward Propagation\"></a><strong>Forward Propagation</strong></h2><p><em><strong><a href=\"https://www.youtube.com/watch?v=UJwK6jAStmg\">Forward Propagation</a></strong></em> refers to the calculation and storage of intermediate variables (including outputs) for a neural network in order from the input layer to the output layer.</p>\n<p>It is how neural networks make predictions. Input data is â€œforward propagatedâ€ through the network layer by layer to the final layer which outputs a prediction.</p>\n<h3 id=\"Steps\"><a href=\"#Steps\" class=\"headerlink\" title=\"Steps\"></a>Steps</h3><ol>\n<li>Calculate the weighted input to the hidden layer by multiplying ğ‘‹ by the hidden weight ğ‘Šâ„</li>\n<li>Apply the input of hidden layer to activation function and pass the result(output of hidden layer) to the final layer</li>\n<li>Repeat step 2 except this time ğ‘‹ is replaced by the hidden layerâ€™s output, ğ»</li>\n</ol>\n<h2 id=\"Cost-Loss-Error-Function\"><a href=\"#Cost-Loss-Error-Function\" class=\"headerlink\" title=\"Cost/Loss/Error Function\"></a><strong><a href=\"https://en.wikipedia.org/wiki/Loss_function\">Cost/Loss/Error Function</a></strong></h2><p>A loss function/error function(defined on a data point, prediction and label, and measures the penalty) is <em><strong>for a single training example/input</strong></em>. A cost function, on the other hand, is <em><strong>the average loss over the entire training dataset</strong></em>, which might be a sum of loss functions over your training set plus some model complexity penalty (regularization).<br>The optimization strategies aim at â€œminimizing the cost functionâ€.</p>\n<ul>\n<li><p><a href=\"https://stats.stackexchange.com/questions/179026/objective-function-cost-function-loss-function-are-they-the-same-thing\">Objective function, cost function, loss function: are they the same thing?</a></p>\n</li>\n<li><p>A loss function is a part of a cost function which is a type of an objective function.</p>\n</li>\n</ul>\n<h2 id=\"Back-Propagation\"><a href=\"#Back-Propagation\" class=\"headerlink\" title=\"Back Propagation\"></a><strong>Back Propagation</strong></h2><p><em><strong><a href=\"https://www.cnblogs.com/charlotte77/p/5629865.html\">Back Propagation</a></strong></em> refers to the method of calculating the gradient of neural network parameters. In short, the method traverses the network in reverse order, from the output to the input layer, according to the chain rule from calculus. The algorithm stores any intermediate variables (partial derivatives) required while calculating the gradient with respect to some parameters.</p>\n<p>The goals of backpropagation are straightforward: <em><strong>adjust each weight in the network in proportion to how much it contributes to overall error</strong></em>. If we iteratively reduce each weightâ€™s error, eventually weâ€™ll have a series of weights that produce good predictions.</p>\n<h3 id=\"Steps-1\"><a href=\"#Steps-1\" class=\"headerlink\" title=\"Steps\"></a>Steps</h3><ol>\n<li>Compare the actual value output by the forward propagation process to the expected value</li>\n<li>Moves backward through the network, slightly adjusting each of the weights in a direction that reduces the size of the error by a small degree</li>\n<li>Both forward and back propagation are re-run thousands of times on each input combination until the network can accurately predict the expected output of the possible inputs using forward propagation.</li>\n</ol>\n<h3 id=\"Formulas-and-derivation\"><a href=\"#Formulas-and-derivation\" class=\"headerlink\" title=\"Formulas and derivation\"></a>Formulas and derivation</h3><ol>\n<li><a href=\"https://blog.csdn.net/u014313009/article/details/51039334?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.pc_relevant_default&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.pc_relevant_default&utm_relevant_index=2\">åå‘ä¼ æ’­ç®—æ³•ï¼ˆè¿‡ç¨‹åŠå…¬å¼æ¨å¯¼ï¼‰</a></li>\n<li><a href=\"https://brilliant.org/wiki/backpropagation/#\">Backpropagation</a></li>\n<li><a href=\"https://www.cnblogs.com/charlotte77/p/5629865.html\">ä¸€æ–‡å¼„æ‡‚ç¥ç»ç½‘ç»œä¸­çš„åå‘ä¼ æ’­æ³•â€”â€”BackPropagation</a></li>\n</ol>\n<h2 id=\"Stochastic-Gradient-Descent-Algorithm\"><a href=\"#Stochastic-Gradient-Descent-Algorithm\" class=\"headerlink\" title=\"Stochastic Gradient Descent Algorithm\"></a><strong>Stochastic Gradient Descent Algorithm</strong></h2><p><em><strong>Gradient descent</strong></em> is an iterative algorithm, that starts from a random point on a function and travels down its slope in steps until it reaches the lowest point of that function.</p>\n<p>Gradient descent <strong>can be slow to run on very large datasets</strong>.</p>\n<p>Because one iteration of the gradient descent algorithm requires a prediction for each instance in the training dataset, it can <strong>take a long time when you have many millions of instances</strong>.</p>\n<p><em><strong>Stochastic Gradient Descent(SGD)</strong></em> is a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). </p>\n<p>In this variation, the gradient descent procedure is run but the update to the coefficients is performed for each training instance, rather than at the end of the batch of instances.</p>\n<p>It is while selecting data points at each step to calculate the derivatives that induces randomness in gradient descent algorithm. SGD randomly picks one data point from the whole data set at each iteration to reduce the computations enormously.</p>\n<ul>\n<li><a href=\"https://www.analyticsvidhya.com/blog/2020/10/how-does-the-gradient-descent-algorithm-work-in-machine-learning/\">How Does the Gradient Descent Algorithm Work in Machine Learning?</a></li>\n<li><a href=\"https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31\">Stochastic Gradient Descent â€” Clearly Explained !!</a></li>\n<li><a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\">Gradient Descent For Machine Learning</a></li>\n</ul>\n<h2 id=\"Activation-Functions\"><a href=\"#Activation-Functions\" class=\"headerlink\" title=\"Activation Functions\"></a><strong>Activation Functions</strong></h2><ul>\n<li><a href=\"https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#linear\">Linear</a></li>\n<li><a href=\"https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#elu\">ELU</a></li>\n<li><a href=\"https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#relu\">ReLU</a></li>\n<li><a href=\"https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#leakyrelu\">LeakyReLU</a></li>\n<li><a href=\"https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#sigmoid\">Sigmoid</a></li>\n<li><a href=\"https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#tanh\">Tanh</a></li>\n<li><a href=\"https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#softmax\">Softmax</a></li>\n</ul>\n<h2 id=\"References-Sources\"><a href=\"#References-Sources\" class=\"headerlink\" title=\"References/Sources\"></a>References/Sources</h2><ol>\n<li><a href=\"https://en.wikipedia.org/wiki/Gradient\">https://en.wikipedia.org/wiki/Gradient</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Gradient_descent\">https://en.wikipedia.org/wiki/Gradient_descent</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/34378516\">ç¥ç»ç½‘ç»œï¼ˆå…¨è¿æ¥ï¼‰çš„å‰å‘å’Œåå‘ä¼ æ’­</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=UJwK6jAStmg\">https://www.youtube.com/watch?v=UJwK6jAStmg</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Loss_function\">https://en.wikipedia.org/wiki/Loss_function</a></li>\n<li><a href=\"https://stats.stackexchange.com/questions/179026/objective-function-cost-function-loss-function-are-they-the-same-thing\">https://stats.stackexchange.com/questions/179026/objective-function-cost-function-loss-function-are-they-the-same-thing</a></li>\n<li><a href=\"https://blog.csdn.net/u014313009/article/details/51039334?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.pc_relevant_default&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.pc_relevant_default&utm_relevant_index=2\">åå‘ä¼ æ’­ç®—æ³•ï¼ˆè¿‡ç¨‹åŠå…¬å¼æ¨å¯¼ï¼‰</a></li>\n<li><a href=\"https://brilliant.org/wiki/backpropagation/#\">Backpropagation</a></li>\n<li><a href=\"https://www.cnblogs.com/charlotte77/p/5629865.html\">ä¸€æ–‡å¼„æ‡‚ç¥ç»ç½‘ç»œä¸­çš„åå‘ä¼ æ’­æ³•â€”â€”BackPropagation</a></li>\n<li><a href=\"https://www.analyticsvidhya.com/blog/2020/10/how-does-the-gradient-descent-algorithm-work-in-machine-learning/\">How Does the Gradient Descent Algorithm Work in Machine Learning?</a></li>\n<li><a href=\"https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31\">Stochastic Gradient Descent â€” Clearly Explained !!</a></li>\n<li><a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\">Gradient Descent For Machine Learning</a></li>\n</ol>\n","raw":null,"categories":[{"name":"Machine-Learning","path":"api/categories/Machine-Learning.json"},{"name":"Deep-Learning","path":"api/categories/Deep-Learning.json"}],"tags":[{"name":"Deep-Learning-Neural-Network","path":"api/tags/Deep-Learning-Neural-Network.json"}]}]}