{"total":4,"pageSize":10,"pageCount":1,"data":[{"title":"Notes on Transfer Learning","slug":"Notes-on-Transfer-Learning","date":"2022-01-22T23:21:34.000Z","updated":"2022-01-22T23:44:27.985Z","comments":true,"path":"api/articles/Notes-on-Transfer-Learning.json","excerpt":null,"keywords":"programmer, photographer, PHB, PhotograpHB","cover":"https://miro.medium.com/max/2000/1*ZEJeJS06czdyPwov5EbCuQ.png","content":"<h1 id=\"Transfer-Learning\"><a href=\"#Transfer-Learning\" class=\"headerlink\" title=\"Transfer Learning\"></a>Transfer Learning</h1><h2 id=\"Concepts\"><a href=\"#Concepts\" class=\"headerlink\" title=\"Concepts\"></a>Concepts</h2><h3 id=\"1-Definition\"><a href=\"#1-Definition\" class=\"headerlink\" title=\"1. Definition\"></a>1. Definition</h3><p><a href=\"https://en.wikipedia.org/wiki/Transfer_learning\">Transfer learning (TL)</a> is a research problem in machine learning (ML) that focuses on <strong>storing knowledge gained while solving one problem and applying it to a different but related problem</strong>.</p>\n<p>It is a machine-learning method where the application of knowledge <strong>obtained from a model used in one task</strong> can be <strong>reused as a foundation point</strong> for another task.</p>\n<ul>\n<li>Ability of a system to recognize and apply knowledge and skills learned in previous domains/tasks to novel domains/tasks.</li>\n</ul>\n<h3 id=\"2-Domain-and-Task\"><a href=\"#2-Domain-and-Task\" class=\"headerlink\" title=\"2. Domain and Task\"></a>2. Domain and Task</h3><ul>\n<li><p>Domain: consists of: a <a href=\"https://en.wikipedia.org/wiki/Feature_space\">feature space</a> X and a <a href=\"https://en.wikipedia.org/wiki/Marginal_distribution\">marginal probability distribution</a> P(X)</p>\n</li>\n<li><p>Task: consists of two components: a label space Y and an objective predictive function f: X â†’ Y</p>\n</li>\n</ul>\n<h2 id=\"Different-Types-of-Transfer-Learning\"><a href=\"#Different-Types-of-Transfer-Learning\" class=\"headerlink\" title=\"Different Types of Transfer Learning\"></a>Different Types of Transfer Learning</h2><p>(1) <strong>Inductive Transfer Learning (å½’çº³å¼è¿ç§»å­¦ä¹ ).</strong> The source and target <em><strong>domains are the same(domainsä¸€æ ·)</strong></em>, however, their <em><strong>tasks are still different(tasksä¸ä¸€æ ·)</strong></em> from each another. The model will use <em><strong>inductive biases</strong></em> from the source domain to help improve the performance of the target task. The source task <em><strong>may or may not contain labeled data</strong></em>, further leading onto the model using multitask learning and self-taught learning.</p>\n<ul>\n<li><strong>multitask learningï¼ˆå¤šä»»åŠ¡å­¦ä¹ ï¼‰</strong>ï¼šsource domainçš„labeledæ•°æ®å¯å¾—ã€‚</li>\n<li><strong>self-taught learningï¼ˆè‡ªå­¦ä¹ ï¼‰</strong>ï¼šsource domainçš„labeledæ•°æ®ä¸å¯å¾—ã€‚</li>\n</ul>\n<p>(2) <strong>Transductive Transfer Learning (ç›´æ¨å¼è¿ç§»å­¦ä¹ ).</strong> The source and target <em><strong>tasks share similarities(tasksç±»ä¼¼)</strong></em>, however, the <em><strong>domains are different(domainsä¸ä¸€æ ·)</strong></em>. The source domain contains a lot of <em><strong>labeled data</strong></em>, whereas there is <em><strong>an absence of labeled</strong></em> data in the target domain, further leading onto the model using domain adaptation.</p>\n<p>Based on the number of domains and tasks, it can also be further divided into two types:</p>\n<ul>\n<li><strong>Domain Adaptationï¼ˆåŸŸé€‚é…ï¼‰</strong>ï¼šä¸åŒçš„domains+single task</li>\n<li><strong>Sample Selection Biasï¼ˆæ ·æœ¬é€‰æ‹©åå·® / Covariance Shiftï¼ˆåæ–¹å·®è½¬å˜ï¼‰</strong>ï¼šsingle domain+single task</li>\n</ul>\n<p>(3) <strong>Unsupervised Transfer Learning (æ— ç›‘ç£è¿ç§»å­¦ä¹ ).</strong> Unsupervised learning is when an algorithm is <em><strong>subjected to being able to identify patterns in data sets that have not been labeled or classified</strong></em>. In this case, the source and target <em><strong>domains are similar(domainsç±»ä¼¼)</strong></em>, however, <em><strong>the tasks are different</strong></em>, where <em><strong>data is unlabeled in both source and target(éƒ½ä¸å¯å¾—)</strong></em>. Techniques such as dimensionality reduction and clustering are well known in unsupervised learning.</p>\n<p>Summarization the different settings and scenarios for each of the above techniques in the following table.</p>\n<figure>\n    <img src=\"https://miro.medium.com/max/2000/1*ZEJeJS06czdyPwov5EbCuQ.png\" alt=\"Types of Transfer Learning Strategies and their Settings\">\n    <figcaption align=\"center\" style=\"font-size: 12px\">Types of Transfer Learning Strategies and their Settings</figcaption>\n</figure>\n\n\n<h2 id=\"What-to-transfer\"><a href=\"#What-to-transfer\" class=\"headerlink\" title=\"What to transfer\"></a>What to transfer</h2><h3 id=\"1-Homogeneous-Transfer-Learning-åŒæ„è¿ç§»å­¦ä¹ \"><a href=\"#1-Homogeneous-Transfer-Learning-åŒæ„è¿ç§»å­¦ä¹ \" class=\"headerlink\" title=\"1. Homogeneous Transfer Learning (åŒæ„è¿ç§»å­¦ä¹ )\"></a>1. <strong>Homogeneous Transfer Learning</strong> (åŒæ„è¿ç§»å­¦ä¹ )</h3><p>Homogeneous Transfer learning approaches are developed and proposed to handle situations where <em><strong>the domains are of the same feature space</strong></em>.</p>\n<p>In Homogeneous Transfer learning, <em><strong>domains have only a slight difference in marginal distributions</strong></em>. These approaches adapt the domains by <em><strong>correcting the sample selection bias or covariate shift</strong></em>.</p>\n<p>(1) <strong>Instance-based transfer</strong>ï¼ˆæ ·æœ¬è¿ç§»ï¼‰</p>\n<p>It covers a simple scenario in which there is <em><strong>a large amount of labeled data in the source domain and a limited number in the target domain</strong></em>. Both the domains and feature spaces <em><strong>differ only in marginal distributions</strong></em>.</p>\n<p>In this scenario, it is natural to consider <em><strong>adapting the marginal distributions</strong></em>. Instance-based Transfer learning <em><strong>reassigns weights to the source domain instances in the loss function</strong></em>.</p>\n<p>Instance reweightingï¼ˆæ ·æœ¬é‡æ–°è°ƒæ•´æƒé‡ï¼‰ and importance samplingï¼ˆé‡è¦æ€§é‡‡æ ·ï¼‰are two main approaches used in instance-based TL.</p>\n<p>(2) <strong>Feature-representation transfer</strong>ï¼ˆç‰¹å¾è¿ç§»ï¼‰</p>\n<p>Feature-based approaches transform the original features to create a new feature representation. This approach can further be divided into two subcategories, i.e., asymmetric and symmetric Feature-based Transfer Learning.</p>\n<ul>\n<li><strong>Asymmetric approaches</strong> transform the source features to match the target ones. In other words, we <em><strong>take the features from the source domain and fit them into the target feature space</strong></em>. There can be some information loss in this process due to the marginal difference in the feature distribution.</li>\n<li><strong>Symmetric approaches</strong> find a common latent feature space and then transform both the source and the target features into this new feature representation.</li>\n</ul>\n<p>(3) <strong>Parameter transfer</strong>ï¼ˆå‚æ•°/æ¨¡å‹è¿ç§»ï¼‰</p>\n<p>The parameter-based transfer learning approaches transfer the knowledge at the <em><strong>model/parameter level</strong></em>.</p>\n<p>This approach involves transferring knowledge through the shared parameters of the source and target domain learner models. One way to transfer the learned knowledge can be <em><strong>by creating multiple source learner models and optimally combining the re-weighted learners similar to ensemble learners to form an improved target learner</strong></em>.</p>\n<p>The idea behind parameter-based methods is that <em><strong>a well-trained model on the source domain has learned a well-defined structure, and if two tasks are related, this structure can be transferred to the target model</strong></em>. In general, there are two ways to share the weights in deep learning models: </p>\n<ul>\n<li><strong>Soft weight sharing</strong>. The model is expected to be close to the already learned features and is usually penalized if its weights deviate significantly from a given set of weights.</li>\n<li><strong>Hard weight sharing</strong>. We share the exact weights among different models.</li>\n</ul>\n<p>(4) <strong>Relational-knowledge transfer</strong>ï¼ˆå…³ç³»è¿ç§»ï¼‰</p>\n<p>Relational-based transfer learning approaches mainly focus on <em><strong>learning the relations between the source and a target domain</strong></em> and <em><strong>using this knowledge to derive past knowledge and use it in the current context</strong></em>.</p>\n<p>Such approaches transfer <em><strong>the logical relationship or rules learned in the source domain to the target domain</strong></em>.</p>\n<p>For example, if we learn the relationship between different elements of the speech in a male voice, it can help significantly to analyze the sentence in another voice.</p>\n<h3 id=\"2-Heterogeneous-Transfer-Learning-å¼‚æ„è¿ç§»å­¦ä¹ \"><a href=\"#2-Heterogeneous-Transfer-Learning-å¼‚æ„è¿ç§»å­¦ä¹ \" class=\"headerlink\" title=\"2. Heterogeneous Transfer Learning (å¼‚æ„è¿ç§»å­¦ä¹ )\"></a>2. <strong>Heterogeneous Transfer Learning</strong> (å¼‚æ„è¿ç§»å­¦ä¹ )</h3><p>It is often challenging to collect labeled source domain data with the same feature space as the target domain, and Heterogeneous Transfer learning methods are developed to address such limitations.  </p>\n<p>This technique aims to <em><strong>solve the issue of source and target domains having differing feature spaces and other concerns like differing data distributions and label spaces</strong></em>. Heterogeneous Transfer Learning is applied in cross-domain tasks such as cross-language text categorization, text-to-image classification, and many others.</p>\n<p>The following table clearly summarizes the relationship between different transfer learning strategies and what to transfer.</p>\n<figure>\n    <img src=\"https://miro.medium.com/max/700/1*xK81ohzG-tLRKVexowUvgw.png\" alt=\"Transfer Learning Strategies and Types of Transferable Components\">\n    <figcaption align=\"center\" style=\"font-size: 12px\">Transfer Learning Strategies and Types of Transferable Components</figcaption>\n</figure>\n\n<figure>\n    <img src=\"https://miro.medium.com/max/611/1*mEHO0-LifV7MgwXSpY9wyQ.png\" alt=\"An overview of different settings of transfer\">\n    <figcaption align=\"center\" style=\"font-size: 12px\">An overview of different settings of transfer</figcaption>\n</figure>\n\n<h2 id=\"Models\"><a href=\"#Models\" class=\"headerlink\" title=\"Models\"></a>Models</h2><h3 id=\"For-computer-vision\"><a href=\"#For-computer-vision\" class=\"headerlink\" title=\"For computer vision\"></a>For computer vision</h3><ol>\n<li>Xception</li>\n<li>VGG16</li>\n<li>VGG19</li>\n<li>ResNet50</li>\n<li>InceptionV3</li>\n<li>InceptionResNetV2</li>\n<li>MobileNet</li>\n<li>MobileNetV2</li>\n<li>DenseNetV2</li>\n<li>DenseNet121</li>\n<li>DenseNet169</li>\n<li>DenseNet201</li>\n<li>NASNetMobile</li>\n<li>NASNetLarge</li>\n</ol>\n<h3 id=\"For-natural-language-processing\"><a href=\"#For-natural-language-processing\" class=\"headerlink\" title=\"For natural language processing\"></a>For natural language processing</h3><ol>\n<li>Universal Sentence Encoder by Google</li>\n<li>Bidirectional Encoder Representations from Transformers (BERT) by Google</li>\n</ol>\n<h3 id=\"For-sound-recognition\"><a href=\"#For-sound-recognition\" class=\"headerlink\" title=\"For sound recognition\"></a>For sound recognition</h3><ol>\n<li>AudioSet</li>\n<li>FreeSound</li>\n<li>SoundWatch</li>\n</ol>\n<h1 id=\"References-sources\"><a href=\"#References-sources\" class=\"headerlink\" title=\"References/sources\"></a>References/sources</h1><ol>\n<li><a href=\"https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a\">A Comprehensive Hands-on Guide to Transfer Learning with Real-World Applications in Deep Learning</a></li>\n<li><a href=\"https://www.v7labs.com/blog/transfer-learning-guide\">A Newbie-Friendly Guide to Transfer Learning</a></li>\n<li><a href=\"https://blog.csdn.net/vvnzhang2095/article/details/79882013?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.pc_relevant_default&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.pc_relevant_default&utm_relevant_index=2\">è¿ç§»å­¦ä¹ â€“ç»¼è¿°</a></li>\n<li>Pan S J, Yang Q. A survey on transfer learning[J]. IEEE Transactions on knowledge and data engineering, 2009, 22(10): 1345-1359.</li>\n<li>Weiss K, Khoshgoftaar T M, Wang D D. A survey of transfer learning[J]. Journal of Big data, 2016, 3(1): 1-40.</li>\n<li>Zhuang F, Qi Z, Duan K, et al. A comprehensive survey on transfer learning[J]. Proceedings of the IEEE, 2020, 109(1): 43-76.</li>\n<li>Carney M, Webster B, Alvarado I, et al. Teachable machine: Approachable Web-based tool for exploring machine learning classification[C]//Extended abstracts of the 2020 CHI conference on human factors in computing systems. 2020: 1-8.</li>\n<li>Goodman S M, Liu P, Jain D, et al. Toward user-driven sound recognizer personalization with people who are d/deaf or hard of hearing[J]. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 2021, 5(2): 1-23.</li>\n<li>Laput G, Ahuja K, Goel M, et al. Ubicoustics: Plug-and-play acoustic activity recognition[C]//Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology. 2018: 213-224.</li>\n<li><a href=\"https://github.com/jindongwang/transferlearning\">https://github.com/jindongwang/transferlearning</a></li>\n<li><a href=\"https://github.com/googlecreativelab/teachable-machine-boilerplate\">https://github.com/googlecreativelab/teachable-machine-boilerplate</a></li>\n</ol>\n","raw":null,"categories":[{"name":"Machine-Learning","path":"api/categories/Machine-Learning.json"},{"name":"Deep-Learning","path":"api/categories/Deep-Learning.json"}],"tags":[{"name":"Transfer-Learning-Machine-Learning-Deep-Learning","path":"api/tags/Transfer-Learning-Machine-Learning-Deep-Learning.json"}]},{"title":"Notes on Neural Network","slug":"Notes-on-Neural-Network","date":"2022-01-22T23:20:43.000Z","updated":"2022-01-22T23:44:22.840Z","comments":true,"path":"api/articles/Notes-on-Neural-Network.json","excerpt":null,"keywords":"programmer, photographer, PHB, PhotograpHB","cover":null,"content":"<h1 id=\"Neural-Network\"><a href=\"#Neural-Network\" class=\"headerlink\" title=\"Neural Network\"></a>Neural Network</h1><h2 id=\"Forward-Propagation\"><a href=\"#Forward-Propagation\" class=\"headerlink\" title=\"Forward Propagation\"></a><strong>Forward Propagation</strong></h2><p><em><strong><a href=\"https://www.youtube.com/watch?v=UJwK6jAStmg\">Forward Propagation</a></strong></em> refers to the calculation and storage of intermediate variables (including outputs) for a neural network in order from the input layer to the output layer.</p>\n<p>It is how neural networks make predictions. Input data is â€œforward propagatedâ€ through the network layer by layer to the final layer which outputs a prediction.</p>\n<h3 id=\"Steps\"><a href=\"#Steps\" class=\"headerlink\" title=\"Steps\"></a>Steps</h3><ol>\n<li>Calculate the weighted input to the hidden layer by multiplying ğ‘‹ by the hidden weight ğ‘Šâ„</li>\n<li>Apply the input of hidden layer to activation function and pass the result(output of hidden layer) to the final layer</li>\n<li>Repeat step 2 except this time ğ‘‹ is replaced by the hidden layerâ€™s output, ğ»</li>\n</ol>\n<h2 id=\"Cost-Loss-Error-Function\"><a href=\"#Cost-Loss-Error-Function\" class=\"headerlink\" title=\"Cost/Loss/Error Function\"></a><strong><a href=\"https://en.wikipedia.org/wiki/Loss_function\">Cost/Loss/Error Function</a></strong></h2><p>A loss function/error function(defined on a data point, prediction and label, and measures the penalty) is <em><strong>for a single training example/input</strong></em>. A cost function, on the other hand, is <em><strong>the average loss over the entire training dataset</strong></em>, which might be a sum of loss functions over your training set plus some model complexity penalty (regularization).<br>The optimization strategies aim at â€œminimizing the cost functionâ€.</p>\n<ul>\n<li><p><a href=\"https://stats.stackexchange.com/questions/179026/objective-function-cost-function-loss-function-are-they-the-same-thing\">Objective function, cost function, loss function: are they the same thing?</a></p>\n</li>\n<li><p>A loss function is a part of a cost function which is a type of an objective function.</p>\n</li>\n</ul>\n<h2 id=\"Back-Propagation\"><a href=\"#Back-Propagation\" class=\"headerlink\" title=\"Back Propagation\"></a><strong>Back Propagation</strong></h2><p><em><strong><a href=\"https://www.cnblogs.com/charlotte77/p/5629865.html\">Back Propagation</a></strong></em> refers to the method of calculating the gradient of neural network parameters. In short, the method traverses the network in reverse order, from the output to the input layer, according to the chain rule from calculus. The algorithm stores any intermediate variables (partial derivatives) required while calculating the gradient with respect to some parameters.</p>\n<p>The goals of backpropagation are straightforward: <em><strong>adjust each weight in the network in proportion to how much it contributes to overall error</strong></em>. If we iteratively reduce each weightâ€™s error, eventually weâ€™ll have a series of weights that produce good predictions.</p>\n<h3 id=\"Steps-1\"><a href=\"#Steps-1\" class=\"headerlink\" title=\"Steps\"></a>Steps</h3><ol>\n<li>Compare the actual value output by the forward propagation process to the expected value</li>\n<li>Moves backward through the network, slightly adjusting each of the weights in a direction that reduces the size of the error by a small degree</li>\n<li>Both forward and back propagation are re-run thousands of times on each input combination until the network can accurately predict the expected output of the possible inputs using forward propagation.</li>\n</ol>\n<h3 id=\"Formulas-and-derivation\"><a href=\"#Formulas-and-derivation\" class=\"headerlink\" title=\"Formulas and derivation\"></a>Formulas and derivation</h3><ol>\n<li><a href=\"https://blog.csdn.net/u014313009/article/details/51039334?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.pc_relevant_default&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.pc_relevant_default&utm_relevant_index=2\">åå‘ä¼ æ’­ç®—æ³•ï¼ˆè¿‡ç¨‹åŠå…¬å¼æ¨å¯¼ï¼‰</a></li>\n<li><a href=\"https://brilliant.org/wiki/backpropagation/#\">Backpropagation</a></li>\n<li><a href=\"https://www.cnblogs.com/charlotte77/p/5629865.html\">ä¸€æ–‡å¼„æ‡‚ç¥ç»ç½‘ç»œä¸­çš„åå‘ä¼ æ’­æ³•â€”â€”BackPropagation</a></li>\n</ol>\n<h2 id=\"Stochastic-Gradient-Descent-Algorithm\"><a href=\"#Stochastic-Gradient-Descent-Algorithm\" class=\"headerlink\" title=\"Stochastic Gradient Descent Algorithm\"></a><strong>Stochastic Gradient Descent Algorithm</strong></h2><p><em><strong>Gradient descent</strong></em> is an iterative algorithm, that starts from a random point on a function and travels down its slope in steps until it reaches the lowest point of that function.</p>\n<p>Gradient descent <strong>can be slow to run on very large datasets</strong>.</p>\n<p>Because one iteration of the gradient descent algorithm requires a prediction for each instance in the training dataset, it can <strong>take a long time when you have many millions of instances</strong>.</p>\n<p><em><strong>Stochastic Gradient Descent(SGD)</strong></em> is a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). </p>\n<p>In this variation, the gradient descent procedure is run but the update to the coefficients is performed for each training instance, rather than at the end of the batch of instances.</p>\n<p>It is while selecting data points at each step to calculate the derivatives that induces randomness in gradient descent algorithm. SGD randomly picks one data point from the whole data set at each iteration to reduce the computations enormously.</p>\n<ul>\n<li><a href=\"https://www.analyticsvidhya.com/blog/2020/10/how-does-the-gradient-descent-algorithm-work-in-machine-learning/\">How Does the Gradient Descent Algorithm Work in Machine Learning?</a></li>\n<li><a href=\"https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31\">Stochastic Gradient Descent â€” Clearly Explained !!</a></li>\n<li><a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\">Gradient Descent For Machine Learning</a></li>\n</ul>\n<h2 id=\"Activation-Functions\"><a href=\"#Activation-Functions\" class=\"headerlink\" title=\"Activation Functions\"></a><strong>Activation Functions</strong></h2><ul>\n<li><a href=\"https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#linear\">Linear</a></li>\n<li><a href=\"https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#elu\">ELU</a></li>\n<li><a href=\"https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#relu\">ReLU</a></li>\n<li><a href=\"https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#leakyrelu\">LeakyReLU</a></li>\n<li><a href=\"https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#sigmoid\">Sigmoid</a></li>\n<li><a href=\"https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#tanh\">Tanh</a></li>\n<li><a href=\"https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#softmax\">Softmax</a></li>\n</ul>\n<h2 id=\"References-Sources\"><a href=\"#References-Sources\" class=\"headerlink\" title=\"References/Sources\"></a>References/Sources</h2><ol>\n<li><a href=\"https://en.wikipedia.org/wiki/Gradient\">https://en.wikipedia.org/wiki/Gradient</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Gradient_descent\">https://en.wikipedia.org/wiki/Gradient_descent</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/34378516\">ç¥ç»ç½‘ç»œï¼ˆå…¨è¿æ¥ï¼‰çš„å‰å‘å’Œåå‘ä¼ æ’­</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=UJwK6jAStmg\">https://www.youtube.com/watch?v=UJwK6jAStmg</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Loss_function\">https://en.wikipedia.org/wiki/Loss_function</a></li>\n<li><a href=\"https://stats.stackexchange.com/questions/179026/objective-function-cost-function-loss-function-are-they-the-same-thing\">https://stats.stackexchange.com/questions/179026/objective-function-cost-function-loss-function-are-they-the-same-thing</a></li>\n<li><a href=\"https://blog.csdn.net/u014313009/article/details/51039334?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.pc_relevant_default&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.pc_relevant_default&utm_relevant_index=2\">åå‘ä¼ æ’­ç®—æ³•ï¼ˆè¿‡ç¨‹åŠå…¬å¼æ¨å¯¼ï¼‰</a></li>\n<li><a href=\"https://brilliant.org/wiki/backpropagation/#\">Backpropagation</a></li>\n<li><a href=\"https://www.cnblogs.com/charlotte77/p/5629865.html\">ä¸€æ–‡å¼„æ‡‚ç¥ç»ç½‘ç»œä¸­çš„åå‘ä¼ æ’­æ³•â€”â€”BackPropagation</a></li>\n<li><a href=\"https://www.analyticsvidhya.com/blog/2020/10/how-does-the-gradient-descent-algorithm-work-in-machine-learning/\">How Does the Gradient Descent Algorithm Work in Machine Learning?</a></li>\n<li><a href=\"https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31\">Stochastic Gradient Descent â€” Clearly Explained !!</a></li>\n<li><a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\">Gradient Descent For Machine Learning</a></li>\n</ol>\n","raw":null,"categories":[{"name":"Machine-Learning","path":"api/categories/Machine-Learning.json"},{"name":"Deep-Learning","path":"api/categories/Deep-Learning.json"}],"tags":[{"name":"Deep-Learning-Neural-Network","path":"api/tags/Deep-Learning-Neural-Network.json"}]},{"title":"Hexo+GitHubPages+è…¾è®¯äº‘æ­å»ºä¸ªäººåšå®¢","slug":"blog-how-to","date":"2022-01-17T14:19:37.000Z","updated":"2022-01-17T15:25:47.554Z","comments":true,"path":"api/articles/blog-how-to.json","excerpt":null,"keywords":"programmer, photographer, PHB, PhotograpHB","cover":null,"content":"<p>æœ¬æ•™ç¨‹éœ€è¦è¯»è€…å…·æœ‰åŸºç¡€çš„ç¼–è¾‘å™¨(VSCode/WebStorm/å…¶ä»–â€¦)å’Œå‘½ä»¤è¡ŒçŸ¥è¯†ï¼Œä»¥åŠæ–‡æ¡£æ•™ç¨‹é˜…è¯»å’Œå¤ç°èƒ½åŠ›ã€‚ä»¥ä¸‹å¤§è‡´è¯´æ˜ä»¥ä¸‹æ­¥éª¤å§ã€‚</p>\n<h2 id=\"Hexo\"><a href=\"#Hexo\" class=\"headerlink\" title=\"Hexo\"></a>Hexo</h2><h3 id=\"å®‰è£…ä¾èµ–é¡¹å’ŒHexo\"><a href=\"#å®‰è£…ä¾èµ–é¡¹å’ŒHexo\" class=\"headerlink\" title=\"å®‰è£…ä¾èµ–é¡¹å’ŒHexo\"></a>å®‰è£…ä¾èµ–é¡¹å’ŒHexo</h3><p>æŒ‰ç…§å®˜æ–¹æ–‡æ¡£å®‰è£…æ‰€éœ€ç¨‹åºï¼ˆNode.js/Gitï¼‰ã€‚</p>\n<p>å®‰è£…æŒ‡å—ä¼ é€é—¨ï¼š</p>\n<ol>\n<li><a href=\"https://www.runoob.com/nodejs/nodejs-install-setup.html\">Node.js</a></li>\n<li><a href=\"https://www.runoob.com/git/git-install-setup.html\">Git</a></li>\n<li><a href=\"https://hexo.io/zh-cn/docs/#%E5%AE%89%E8%A3%85-Hexo\">Hexo</a></li>\n</ol>\n<h3 id=\"ä½¿ç”¨Hexoå»ºç«™å¹¶ä¿®æ”¹é…ç½®\"><a href=\"#ä½¿ç”¨Hexoå»ºç«™å¹¶ä¿®æ”¹é…ç½®\" class=\"headerlink\" title=\"ä½¿ç”¨Hexoå»ºç«™å¹¶ä¿®æ”¹é…ç½®\"></a>ä½¿ç”¨Hexoå»ºç«™å¹¶ä¿®æ”¹é…ç½®</h3><p>è¯¦è§å®˜æ–¹æ–‡æ¡£ï¼š</p>\n<ol>\n<li><a href=\"https://hexo.io/zh-cn/docs/setup\">å»ºç«™</a></li>\n<li><a href=\"https://hexo.io/zh-cn/docs/configuration\">ä¿®æ”¹é…ç½®</a></li>\n</ol>\n<h3 id=\"å¯åŠ¨ç½‘ç«™\"><a href=\"#å¯åŠ¨ç½‘ç«™\" class=\"headerlink\" title=\"å¯åŠ¨ç½‘ç«™\"></a>å¯åŠ¨ç½‘ç«™</h3><p>åœ¨å‘½ä»¤è¡Œè¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n\n<p>ç„¶åå†æµè§ˆå™¨è¾“å…¥ <a href=\"http://localhost:4000/\">http://localhost:4000/</a> å°±å¯ä»¥æ‰“å¼€ç½‘ç«™çœ‹åˆ°åŸå§‹ç‰ˆçš„ä¸ªäººåšå®¢ç½‘ç«™äº†ã€‚</p>\n<h2 id=\"GitHub-Pages\"><a href=\"#GitHub-Pages\" class=\"headerlink\" title=\"GitHub Pages\"></a>GitHub Pages</h2><p>è¿™ä¸€éƒ¨åˆ†éœ€è¦ä½ æœ‰ä¸€ä¸ªgithubçš„è´¦å·ï¼Œä»¥åŠæ–°å»ºä¸€ä¸ªrepoï¼Œå¹¶ä¸”å°†ä¸Šä¸€æ­¥Hexoä¸­çš„ä»£ç å…³è”åˆ°è¿™ä¸ªrepoã€‚</p>\n<h3 id=\"åˆ›å»ºGitHub-Pagesç«™ç‚¹\"><a href=\"#åˆ›å»ºGitHub-Pagesç«™ç‚¹\" class=\"headerlink\" title=\"åˆ›å»ºGitHub Pagesç«™ç‚¹\"></a>åˆ›å»ºGitHub Pagesç«™ç‚¹</h3><p>æŒ‡å—ä¼ é€é—¨ï¼š<a href=\"https://docs.github.com/cn/pages/getting-started-with-github-pages/creating-a-github-pages-site\">åˆ›å»ºGitHub Pagesç«™ç‚¹</a></p>\n<h3 id=\"ä¿®æ”¹åšå®¢url\"><a href=\"#ä¿®æ”¹åšå®¢url\" class=\"headerlink\" title=\"ä¿®æ”¹åšå®¢url\"></a>ä¿®æ”¹åšå®¢url</h3><p>åœ¨_config.ymlæ–‡ä»¶ä¸­ä¿®æ”¹urlä¸ºä½ çš„repoçš„Github Pagesçš„ç½‘å€ï¼š</p>\n<figure class=\"highlight yml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># URL</span></span><br><span class=\"line\"><span class=\"comment\">## Set your site url here. For example, if you use GitHub Page, set url as &#x27;https://username.github.io/repo&#x27;</span></span><br><span class=\"line\"><span class=\"attr\">url:</span> <span class=\"string\">https://username.github.io/repo</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"éƒ¨ç½²åˆ°GitHub-Pages\"><a href=\"#éƒ¨ç½²åˆ°GitHub-Pages\" class=\"headerlink\" title=\"éƒ¨ç½²åˆ°GitHub Pages\"></a>éƒ¨ç½²åˆ°GitHub Pages</h3><ol>\n<li>å®‰è£… hexo-deployer-gitã€‚</li>\n</ol>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure>\n\n<ol start=\"2\">\n<li><p>ä¿®æ”¹é…ç½®ï¼š</p>\n<figure class=\"highlight yml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">deploy:</span></span><br><span class=\"line\">  <span class=\"attr\">type:</span> <span class=\"string\">git</span></span><br><span class=\"line\">  <span class=\"attr\">repo:</span> <span class=\"string\">&lt;repository</span> <span class=\"string\">url&gt;</span> <span class=\"comment\">#repoçš„åœ°å€ï¼Œå¦‚https://github.com/username/repo</span></span><br><span class=\"line\">  <span class=\"attr\">branch:</span> [<span class=\"string\">branch</span>] <span class=\"comment\">#éœ€è¦éƒ¨ç½²çš„ä»£ç åˆ†æ”¯ï¼Œå¦‚mainã€masterï¼Œéœ€è¦å»æ‰[]</span></span><br><span class=\"line\">  <span class=\"attr\">message:</span> [<span class=\"string\">message</span>] <span class=\"comment\">#éƒ¨ç½²ä¿¡æ¯ï¼Œå¯ä»¥éšä¾¿å¡«ï¼Œéœ€è¦å»æ‰[]</span></span><br></pre></td></tr></table></figure></li>\n<li><p>ç”Ÿæˆç«™ç‚¹æ–‡ä»¶å¹¶æ¨é€è‡³è¿œç¨‹åº“</p>\n</li>\n</ol>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo clean &amp;&amp; hexo deploy</span><br></pre></td></tr></table></figure>\n\n<ol start=\"4\">\n<li>åœ¨åº“è®¾ç½®ï¼ˆRepository Settingsï¼‰ä¸­å°†é»˜è®¤åˆ†æ”¯è®¾ç½®ä¸º_config.ymlé…ç½®ä¸­çš„åˆ†æ”¯åç§°å°±å¯ä»¥äº†ã€‚</li>\n</ol>\n<p>è¿™æ ·ä½ å°±å¯ä»¥ç”¨GitHub Pagesçš„urlæ‰“å¼€ä½ çš„ä¸ªäººåšå®¢äº†ã€‚</p>\n<p>æ›´è¯¦ç»†çš„æŒ‡å—è¯·ç§»æ­¥å®˜æ–¹æ–‡æ¡£: <a href=\"https://hexo.io/zh-cn/docs/one-command-deployment\">ä¸€é”®éƒ¨ç½²åˆ°GitHub</a></p>\n<h2 id=\"è…¾è®¯äº‘\"><a href=\"#è…¾è®¯äº‘\" class=\"headerlink\" title=\"è…¾è®¯äº‘\"></a>è…¾è®¯äº‘</h2><h3 id=\"åŸŸåç”³è¯·\"><a href=\"#åŸŸåç”³è¯·\" class=\"headerlink\" title=\"åŸŸåç”³è¯·\"></a>åŸŸåç”³è¯·</h3><p>è´­ä¹°ç”³è¯·æŒ‡å—è¯·ç§»æ­¥ï¼š<a href=\"https://dnspod.cloud.tencent.com/\">è…¾è®¯äº‘è´­ä¹°åŸŸå</a></p>\n<p>å…¶ä¸­éœ€è¦ä¸€å †å®åè®¤è¯è¿‡ç¨‹ï¼Œè¯·æŒ‰ç…§å®˜æ–¹æ–‡æ¡£æ“ä½œã€‚</p>\n<h3 id=\"ç»‘å®šåˆ°Github-Pages\"><a href=\"#ç»‘å®šåˆ°Github-Pages\" class=\"headerlink\" title=\"ç»‘å®šåˆ°Github Pages\"></a>ç»‘å®šåˆ°Github Pages</h3><p>è¯¦è§è¿™ç¯‡åšå®¢: <a href=\"https://cloud.tencent.com/developer/article/1421879\">GitHub Pagesæ­å»ºçš„åšå®¢ç»‘å®šåŸŸå</a></p>\n<p>æ·»åŠ DNSè§£æéƒ¨åˆ†è¯¦è§è¿™ç¯‡ï¼Œéœ€è¦ç”¨å‘½ä»¤è¡Œpingä¸€ä¸‹è‡ªå·±çš„github pageè·å–ipåœ°å€ï¼š<a href=\"https://cloud.tencent.com/developer/article/1454059\">github pagesç»‘å®šåŸŸå</a></p>\n<h3 id=\"ä¿®æ”¹å…ˆå‰çš„é…ç½®\"><a href=\"#ä¿®æ”¹å…ˆå‰çš„é…ç½®\" class=\"headerlink\" title=\"ä¿®æ”¹å…ˆå‰çš„é…ç½®\"></a>ä¿®æ”¹å…ˆå‰çš„é…ç½®</h3><p>åœ¨ç¬¬äºŒæ­¥ä¸­å°†åšå®¢urlè®¾ç½®æˆäº†è‡ªå·±çš„GitHub Pagesçš„urlï¼Œè€Œæ—¢ç„¶æˆ‘ä»¬å·²ç»æœ‰äº†è‡ªå·±çš„åŸŸåï¼Œå°±éœ€è¦æŠŠä¹‹å‰çš„urlæ¢æˆæˆ‘ä»¬è‡ªå·±çš„åŸŸåï¼š</p>\n<figure class=\"highlight yml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># URL</span></span><br><span class=\"line\"><span class=\"comment\">## ä½ è‡ªå·±ç”³è¯·çš„åŸŸå</span></span><br><span class=\"line\"><span class=\"attr\">url:</span> <span class=\"string\">https://username.cn</span></span><br></pre></td></tr></table></figure>\n\n<p>å¤§åŠŸå‘Šæˆã€‚</p>\n<h2 id=\"Hexoä¸»é¢˜\"><a href=\"#Hexoä¸»é¢˜\" class=\"headerlink\" title=\"Hexoä¸»é¢˜\"></a>Hexoä¸»é¢˜</h2><p>å¦‚æœä½ è§‰å¾—é»˜è®¤ä¸»é¢˜ä¸å¤ªå¥½çœ‹ï¼Œæ˜¯å¯ä»¥è‡ªå·±è®¾è®¡æˆ–è€…æ›´æ¢ä¸»é¢˜çš„ã€‚</p>\n<p>å®˜æ–¹æ–‡æ¡£ï¼š<a href=\"https://hexo.io/zh-cn/docs/themes\">ä¸»é¢˜</a></p>\n<p>å®˜æ–¹è¿˜æœ‰ä¸€ä¸ª<a href=\"https://hexo.io/themes/\">ä¸»é¢˜åº“</a>ï¼Œå¯ä»¥æœç´¢è‡ªå·±è§‰å¾—å¥½çœ‹çš„ä¸»é¢˜ï¼Œç„¶åå»ä¸»é¢˜çš„GitHub repoä¸‹è½½å¹¶ç§»åˆ°è‡ªå·±åšå®¢çš„themeæ–‡ä»¶å¤¹ä¸‹ï¼Œä¿®æ”¹å¯¹åº”é…ç½®ä¸ºè¿™ä¸ªä¸»é¢˜çš„åå­—ã€‚</p>\n<p>æ¯”å¦‚æˆ‘ç”¨çš„ä¸»é¢˜ä¸º[ocean](<a href=\"https://github.com/zhwangart/hexo-theme-ocean.git\">https://github.com/zhwangart/hexo-theme-ocean.git</a> themes/ocean)ï¼Œå°±åœ¨root _config.yml ä¸­é€‰æ‹© theme: oceanï¼š</p>\n<figure class=\"highlight yml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">theme:</span> <span class=\"string\">ocean</span></span><br></pre></td></tr></table></figure>\n\n<p>å…¶ä»–ä¸ªæ€§åŒ–é…ç½®å°±éœ€è¦å‚è€ƒæ¯ä¸ªä¸»é¢˜çš„ç›¸å…³æ–‡æ¡£äº†ï¼Œä¸€èˆ¬æ˜¯ä¸»é¢˜çš„è®¾è®¡å¼€å‘è€…å†™çš„ï¼Œæ²¡æœ‰çš„è¯å°±éœ€è¦è‡ªå·±æ‘¸ç´¢äº†ã€‚ä¸»é¢˜é‡Œçš„å„ç§å›¾æ ‡ï¼Œå›¾ç‰‡ï¼Œè§†é¢‘ç­‰ç­‰éƒ½æ˜¯å¯ä»¥è‡ªå·±æ¢çš„ï¼Œå¯èƒ½éœ€è¦ä½¿ç”¨è€…æœ‰ä¸€äº›cssã€htmlå’Œmarkdownçš„åŸºç¡€ï¼Œè¿™é‡Œå°±ä¸æ·±å…¥ä¸‹å»äº†ï¼Œæœ‰å…´è¶£çš„è¯å„ä½è‡ªå·±å­¦ä¹ å§å“ˆå“ˆå“ˆã€‚</p>\n","raw":null,"categories":[{"name":"Hexo-ä¸ªäººåšå®¢","path":"api/categories/Hexo-ä¸ªäººåšå®¢.json"}],"tags":[{"name":"Hexo-ä¸ªäººåšå®¢","path":"api/tags/Hexo-ä¸ªäººåšå®¢.json"}]},{"title":"Hello World","slug":"hello-world","date":"2022-01-16T15:41:10.384Z","updated":"2022-01-17T14:18:35.552Z","comments":true,"path":"api/articles/hello-world.json","excerpt":null,"keywords":"programmer, photographer, PHB, PhotograpHB","cover":null,"content":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n","raw":null,"categories":[{"name":"hello","path":"api/categories/hello.json"}],"tags":[{"name":"hello","path":"api/tags/hello.json"}]}]}